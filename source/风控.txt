
登录 | 注册
关闭

巫山云的专栏
keep calm and carry on
目录视图摘要视图订阅
异步赠书：9月重磅新书升级，本本经典           程序员9月书讯      每周荐书：ES6、虚拟现实、物联网（评论送书）
 小米风控实践
标签： 支付研发数据风控
2017-03-31 11:08 5948人阅读 评论(0) 收藏 举报
 分类： 国际贸易与风控（4）  
目录(?)[+]
风控系统从0到1的过程，极具借鉴意义
邓文俊，小米高级研发工程师，2013 年加入小米，参与了数据后台，风控系统，支付等系统的研发工作。

我来自小米支付，今天分享的主题是小米风控实践。为什么选风控这个题目？其实在我看来风控对互联网金融来说，风控就像是内裤，每个人都有，但是不会把它秀给别人看。对其他人来说风控是很神秘，今天带给大家看看小米我们怎么是一步步走过来，在风控方面做了哪些工作。 小米风控可以分为三个历程，尝试、发展和扩展。



一、尝试

2014 年的时候我们发行我们米币短信充值渠道坏账率非常高，高到我们几乎赚不到钱。米币像 QQ 一样，是虚拟币，用米币可以在小米生态圈内购买所有服务。短信什么渠道呢？用户用短信发一条短信给运营商，运营商就会从它的帐户里面扣钱转给我们。运营商每个月给我们结一次钱，他会告诉我们有一些帐是坏账，他不会把钱给我们。他也不会告诉你哪些是坏账，哪些是好帐。我们要自己想办法解决坏账问题，让自己有钱可赚。

我们设定两种风控规则，这是第一种规则，我们叫限额、限次、限频规则。设备、帐户、手机号是一个标识类。另外一个纬度是限制类，限制交易范围，比如时间、业务、渠道等等。最后还有限制方法。我们的版本里面一个规则包括三个部分，一个是标识，一个是范围，一个是限制方法。



一个帐户一天之内他的消费金额上限是 100 米币。还有一种情况是策略，大家支付都是用手机，这个设备每小时某个业务交易次数可以限制。还有交易的频率，比如一个帐户这次交易跟上次交易要超过，不然很可能是机器行为。一般用户不会频繁交易，除非你是玩游戏非常上瘾，一般不会出现这样情况。

第二类规则限制是标识属性之间的关系，比如我们一般认为一个帐户只会登录到有限几个设备，不会登录太多。反过来能够登录设备也是有性质，要排除测试情况。有了这两个规则之后，我们迅速开始开发，以互联网的模式用了四周就迅速开发，完成了这样的规则引擎在控制台，控制台可以修改规则也可以看到实时效果。开发完了之后我们没有立刻上线，先试运行 2 周，过程是什么？



试运行过程中规则并没有真正的生效。只记录一些交易，会被哪些拦截不成功，有了这些数据之后，我们可以条件规则这些顺序以及阈值，使我们的规则达到最佳效果。运行两周之后有信心，才能真正起来。

上线之后的效果，首先上线一个月之后，运营商一个月之后给我们结钱。一个月之后我们才知道我们的风控效果。上线之后我们发现我们的坏账率比之前最高峰的时候低 1/3。比历史上的坏账率低 1/2。我们渠道成本占到 1/2，我们可以赚钱了。我们一个月的收入多了 50% 左右的增长。

通过这次风控实践尝试，我们有一些经验。第一不要小看简单规则，我们开始的时候我们认为最简单，效果相当好。风控是平衡的，我们目标是达到一种平衡。我们的交易额和我们坏账率之间达到平衡，使我们收益率是最高的。为了达到这个平衡，我们需要不断的利用云的手段，分析数据，调整规则，这是我们决定风控成败很重要的关键。

2014 年底，我们有很多小米用户，如何给小米用户打分，可以是信用分，风险分。信用分可以用于信贷，风险分可以用交易时风险控制。数据有限，我们用米币的形式做模型。



我们用三个纬度建立模型：

还款能力 ，包括用户最近三个月的消费额、消费次数以及累计消费次数还有他的消费情况。比如他在游戏、阅读、视频等不同业务说消费情况怎么样。
依赖性 ，对小米生态圈多么依赖，会不会上瘾，如果走了会不会很痛苦。我们在依赖性方面选取他的注册时间，活跃度，每天花多长时间，还有他使用 App 的情况，最长使用 App 有哪些，频率怎么样。
正常度 ，我们看这个人绝大部分正常，也有不正常。我们用之前积累，使用等待充值频率，等待有风险。类似这样的属性，我们给用户进行打分，这个是打分的图，横轴是依赖性，纵轴往上是它的还款能力。X 轴是依赖性，Y 轴是还款能力，Z 轴是正常度。
（点击图片全屏缩放）

大家可以看到用户分布是非常集中的。我们本来想通过这个图能不能发现一批还款能力、依赖性和正常度都比较高的用户，最后可以做我们的优质用户，作为未来信贷基础。但是很遗憾，我们数据有限，我们没有找到这样的用户。我们只是发现一个事实，我们绝大多数的用户都是还款能力和依赖度很高的正常用户。

二、发展

我们再看 2015 年之后，我们开始了开发自己的第三方支付。我们为小米网、小米生活等小米生态内的其他业务进行金融服务。我介入服务越来越多，我们发现我们遇到的问题越来越多，我们有时候会遇到突发事件，有了方案之后我需要能够迅速上线一些风控规则。因为我们发现每晚一天可能我们的损失都是百万级，希望快速创建风控规则。

业务越来越多，规则越来越多，规则之间的次序都是有严格的要求。制定规则越来越麻烦，越来越容易出错，如何解决这两个问题？我们最后选择了一个开源 Drools，第一它是开源可以免费使用，第二它是 Java 可以很方便扩展，开发自如，进入自己的系统，第三它非常齐全。



这是 Drools 的例子，规则分为三部分，第一是规则名称和它的一些配置属性。第二部分是描述什么事件下会触发这个规则。第三是规则理由。

基于 Drools，我们重新搭建了我们的风控系统，风控系统分成了三个部分，普通规则、CEP 规则和管理后台。



CEP 规则引擎和普通规则引擎区别是它是一种状态，能够缓存数据。刚才大家看到规则，数据实施同步到规则引擎中去。

使用了新的风控系统以后，我们的开发效率和我们运营效率明显提升。5 分钟上线规则，迅速止损。新规则上线到上线调整一周到一天。现在规则非常简单，他们也可以来调整规则。业务同学可以调整规则。

当然规则的调整变容易之后，也是把双刃剑，带来一些其他的问题，有时候规则上线，没有经过严格测试，也没有注意观察规则上的效果，只出现过一两次的事故带来一些小的损失。我们之后为了解决这个问题，我们重新搭建了一个灰度风控系统，这个系统上上线新的规则，利用历史订单验证规则效果，验证成功测试再把放在上面去，这个效果非常好。

风控打点，应用系统要主动调用风控，所以风控在业务系统中需要打点，打点多少位置有技巧，太多风控系统有很多数据低于自己的分析，对业务系统来说非常麻烦。我们采取一般打两个点够了，第一是交易前创建订单之前进行第一次调用，第二次是订单完成之后，只需要调用两次就可以了。

虽然我们有了新的风控系统之后，开发效率、研发效率提高，运行效率没有达到我们期望。我们因为风控系统要为业务系统提供服务，风控系统响应时间过长，会影响业务响应时间，会影响用户的体验。第二，我们现在专门的数据缓存，我们数据都放在 CEP 系统中，拿不出来，无法做后续分析。为了解决这两个问题，替换 CEP 系统，自建一个数据累计服务搜集数据。并且对于非实时性其他数据，我们采用日志方式，离线去进行分析。



上图是我们之前的风控系统，下图是我们修改之后的风控系统。同时在服务里面把数据导出来实时监控每天每分钟，我们整天的拦截率。



我们的风控系统响应时间缩短为以前的 1/4，并且能够实时监控规则运行效果。

（点击图片全屏缩放）

可以看到将数据和计算分离能够解耦带来巨大的好处，另外一方面数据应该尽快用起来，不要只是放在那里，尽快用起来，迭代起来，能够通过反馈搜集更高效的数据。

我们完成了风控系统建立之后，我们回到问题的本质，另外一个问题就是除了拍脑袋，还有其他的规则制定方法吗？我们发现我们其实可以利用机器学习的方式，从案例库中间学习模型，学习规律。同时对于案例库中已知的坏人，我们通过关系找到它的同伙。

从案例库学习方面，我们从案例库中和线上正常交易库中随即挑选两组数据。分别形成我们的训练集和测试集，选取交易数据 17 个交易特征：交易时间、交易者年龄、交易者所使用地理位置、注册地理位置、手机号归属地是否一致等，寻求这些特征，用了四组分类器，在测试集、训练集上进行训练，得到四组模型。最后再用测试集比较这四种模型优劣，最终 我们选择随机森林模型 。

找关系，我们利用交易数据里面用户、设备、手机之间关联关系建立一个关联图，里面很多关联子图。



从案例库中导入相关的坏人的用户和设备，找到有关系的设备和用户，找到之后，在检查这些设备帐户交易特征。如果发现是坏人就把它加入到黑名单。通过这种方式我们预防了盗刷案件。在随机森林模型上线五天之后，我们发现好几笔盗刷，丰富了黑名单 。从损失和错误中学习，把我们的损失变成我们财富。

随着业务快速发展，我们的经验越来越多，我们积累的案例库越来越多。我们发现很多风控问题一个很重要的问题本质是我们的交易对手是谁？他是不是本人？是不是真人？还是一条狗。我们解决这个问题考虑到我们可以利用小米生态里面海量全面的各种各样的用户数据。我们这里面能够知道一个小米帐号他是男是女，兴趣爱好是什么，年龄，学历等等这方面都是有一个画像的。



（点击图片全屏缩放）

这是我们画像组提供一个页面，这里面他把用户分成用户画像、设备画像。设备画像里面包含了 793 条各种关键属性，我们利用其中设备属性以及消费属性建立相应的规则来检查用户的真实性。这样做了以后，我们的 盗刷案件的比例近几个月来下降 40% 。

三、扩展

给我们的启发是他山之石可以攻玉。最后 2016 年以来小米的业务不断扩充，除了之以外，我们还陆续开展贷款、理财、保险、分期等等等等各种各样的业务。这些业务都需要风控，在小米体系下如何共享资源，建立风控体系？

我们解决思路是 对内共享所有数据，对外分享合作 。首先 CTO 牵头下我们建立一个基础风控组，在小米内部协调资源。如果我们需要建立服务的时候，我们建一份，实名认证服务。一个组建立这个服务，分享给其他组使用。在外部合作方面，一个团队有同盾和前海，也可以分享给同事使用。

小米这几年在金融方面扩展非常快，我们陆续开发各种各样业务，我们也希望这一方面牛人加入我们。谢谢大家。

Q&A

Q：做风控，你们利用一些简单 Drools，这个 Drools 包括什么？这个 Drools 怎么定的？后续你们用的一个随机森林框架，怎么把 Drools 放在这随机森林里做的。效果提高了大概 40% 左右，您更详细的介绍一下。如果不方面展开，能否简单举两个例子。

邓文俊：比如盗刷，有地理位置，他交易的时候他有一个地理位置。我们知道他注册的时候有地理位置，他手机号归属地有地理位置，他的身份证也是有地理位置。他最近三个月使用的设备在哪登录过，这些都是可以做限制。

Q：你们是怎么在风控模型中选用随机森林这个模型，底层是不是用了什么开源技术或者一些开源学习框架？

邓文俊：我们所有东西都是用开源的，怎么用随机森林？现在所有东西是基于刚才看到框架叫风控 Drools 框架，它的规则引擎可以调用外面服务或者外面资源。可以由随机森林模型，可以由别的模型，把每个模型学到的东西都可以做成一个独立的服务或者合到一个服务里面。在规则里面调用这个服务，把参数一些背景信息告诉这个服务，这个服务判断或者是有多少的概率是危险。再由规则本身来决定。我们是由规则引擎进行推理，其他所有服务只提供一些参考，最后规则服务来最终决定。事先学习好之后，以服务形式加载起来，放到内存起来，来调用。

Q：训练的时候这个有多少个？

邓文俊：有 17 个。

Q：刚刚我看到你们规则引擎开发是四周就可以上线，你们团队规模以及你们现在运营团队是大概什么样的配比？你们进行开源技术选型的时候所考虑这些方面是什么？

邓文俊：

当时风控就两个人，我开发，还有一个定规则。
风控分三个部门，研发、运营、数据，大概二三十人。到了第三方支付牌照之后可以为外部服务。支付宝据我所知至少一千人。
其实主要考虑方面就是一个，第一安全大家都用过，这是最重要。其次是不是很好用很好上手，文档齐全不齐全。第一很重要，是不是用过，是可靠的。
Q：您刚才说一开始的模型和后来的模型，模拟的模型是一直在迭代吗？

邓文俊：可以调整阈值，没有用高大上的东西。

Q：我看你之前模型一直在，后来走到机器学习，也是四种不同分类器，您选了随机森林。机器学习里面有一个问题，过拟合的问题怎么解决？

邓文俊：不解决，随它去吧。快速开发，快速上线，看迭代。

Q：您现在算法也是不断迭代过程当中，可能有一些问题没有暂时解决是吧？

邓文俊：说实话，我自己感觉我们没有过拟合的问题。

Q：您采用机器学习算法，有一个问题机器学习如果你样本集和训练集给到合适的划分，划分过程中、训练过程中，如果划分不合适或者算法有时候像过拟合抖动没有到合适地步，可能就会慢慢继续拟合，导致没有没有办法恢复。您在机器算法上面现在走的方向上有什么和大家有一些在互联网金融方向特殊的一些设计吗？

邓文俊：目前没有，我们也是刚刚开使用机器学习的方法，摸着石头过河，走一步看一步。如果我要回答你的问题的话，我给的建议是我们做的时候在企业里面其实不像高校管太多，设计那么完美或者会考虑那么多过拟合方面的科研问题、学术问题。我们先上再说，看效果，适者生存。我们并没有太关注过拟合的情况，算出什么结果就拿来用，有效，我们发现盗刷，发现有用，可以再调整。

Q：您刚刚说您样本的这些特征都是通过小米生态平台获得，包括地理位置，可能是一些帐单信息。现在有一个比较争议的问题，您获得这些纬度和用户隐私政策上咱们有什么思路？

邓文俊：小米已成为国内首家和 TRUSTe 公司就用户隐私保护展开合作的手机厂商。我们有两个原则， 第一、收集数据时，收集任何用户隐私数据都是要获得用户允许 ，默认是不许可的，需要用户明确授权。 第二、使用数据时，数据平台保证用户数据是机器可读，人不可读。 我们做分析的时候我们有一个样本的数据做模型，做测试。做完了以后，我们把我们的系统代码或者服务掉到线上跑得时候才是跑的真实的数据库。原来都是看不到，只有机器可以读，人是不可以读。

Q：我本身是做大数据研发，我对金融这方面感兴趣，我又读了金融 MBA。您说研发风控有一些系统，开始的时候是一个风控机组给你说一个业务去做。是不是一个人把金融和 IT 技术都掌握的比较好，把这个系统研发好一些，一个人就专业就建立一个好的团队，大家去合作比较好一些？

邓文俊：这个问题超出技术范畴，从我的角度我自己的体验，我觉得首先应该把自己的事情做好，你有余力可以了解其他人的合作。


顶
0
踩
0
 
 
上一篇终身机器学习（Lifelong Machine Learning）综述
下一篇五大存储模型关系模型、键值存储、文档存储、列式存储、图形数
  相关文章推荐
? 小米风控实践
? 自然语言处理在“天猫精灵”的实践应用--姜飞俊
? 小米深度学习平台架构与实践
? 蚂蜂窝大数据平台架构及Druid引擎实践--汪木铃
? CSDN在线培训：HBase在小米中的应用实践
? Retrofit 从入门封装到源码解析
? 2013年中国数据库大会-04-小米hadoop/hbase微实践
? 程序员如何转型AI工程师
? 小米网架构变迁实践
? 深入探究Linux/VxWorks的设备树
? 小米网抢购系统开发实践
? 使用QEMU搭建u-boot+Linux+NFS嵌入式开发环境
? 第三方推送（小米华为友盟）接入实践
? 小米网抢购系统开发实践和我的个人观察
? 小米网技术架构变迁实践
? 【机器学习PAI实践四】如何实现金融风控

查看评论

  暂无评论

您还没有登录,请[登录]或[注册]
* 以上用户言论只代表其个人观点，不代表CSDN网站的观点或立场
个人资料
 访问我的空间 
EthanSheng
 
访问：163115次
积分：2331
等级： 
排名：第16762名
原创：51篇转载：117篇译文：0篇评论：8条
文章搜索

搜索
文章分类
学习笔记(94)
python笔记(10)
Oracle常见问题(13)
计算机安全(1)
国际贸易与风控(5)
数据挖掘与分析(19)
Linux(5)
数据可视化(21)
深度学习(0)
文章存档
2017年09月(5)
2017年07月(5)
2017年06月(3)
2017年05月(3)
2017年04月(1)
展开
阅读排行
Guided Image Filtering(11061)
小米风控实践(5948)
ORA-00932: 数据类型不一致: 应为 -, 但却获得 CLOB(5520)
sas中retain语句的作用(4333)
failed to load resource the server responded with a status of 500 (internal server error)(4330)
SAS定义宏变量三种方法(3410)
SAS9.3 64位版Win7安装指引(3345)
字符串连接超长的解决wmsys.wm_concat()(2798)
SAS宏技术中,%let和call symput有什么区别？(2793)
Python中通过csv的writerow输出的内容有多余的空行(2590)
评论排行
Guided Image Filtering(4)
failed to load resource the server responded with a status of 500 (internal server error)(2)
软件工程的未来发展趋势(1)
如何在SAS中直接使用Oracle特有函数--sas Pass-Through Facility(1)
C++学习笔记--标准输入输出(0)
AI在蚂蚁金服产品线中的大规模应用(0)
c++学习笔记--显示类型转换(0)
c++学习笔记--指针和const限定符(0)
c++学习笔记--指针和引用的比较(0)
C++学习笔记--标准库类型（vector，iterator，bitset）(三)(0)
推荐文章
* CSDN新版博客feed流内测用户征集令
* Android检查更新下载安装
* 动手打造史上最简单的 Recycleview 侧滑菜单
* TCP网络通讯如何解决分包粘包问题
* SDCC 2017之大数据技术实战线上峰会
* 快速集成一个视频直播功能
最新评论
failed to load resource the server responded with a status of 500 (internal server error)
EthanSheng: 如文章所述，network窗口可以看到返回的报错信息；代码不同，报错的信息，解决方法肯定也不一样
failed to load resource the server responded with a status of 500 (internal server error)
qq_26920399: 遇到同样的问题，请问如何解决的
Guided Image Filtering
青-吾道乐途: @sunxiuqiao09:何凯明博士主页http://kaiminghe.com/
Guided Image Filtering
sunxiuqiao09: @STAR_RUB:请问在哪儿找那个博士论文？
Guided Image Filtering
STAR_RUB: 谢谢博主的分享~对于 咱这种完全不懂的新手来说 很有帮助，另外对于想看原论文 但找不到免费的同学们，...
软件工程的未来发展趋势
dyc_09: 好文章，赞！
Guided Image Filtering
xiaowei_cqu: 效果很像Bilateral filter，有油画的感觉。
如何在SAS中直接使用Oracle特有函数--sas Pass-Through Facility
AvalonZST: 棒啊，如果SAS自身能提供这样的函数就更好了.....


公司简介|招贤纳士|广告服务|联系方式|版权声明|法律顾问|问题报告|合作伙伴|论坛反馈
网站客服杂志客服微博客服webmaster@csdn.net400-660-0108|北京创新乐知信息技术有限公司 版权所有|江苏知之为计算机有限公司|江苏乐知网络技术有限公司
京 ICP 证 09002463 号|Copyright ? 1999-2017, CSDN.NET, All Rights Reserved GongshangLogo

CSDN首页
学院
下载
更多
下载 CSDN APP
写博客
登录|注册
csdn首页移动开发架构云计算/大数据互联网运维数据库前端编程语言研发管理综合全部 
大数据风控案例（总结他人）
原创 2016年11月16日 12:04:38 165100
一、行业背景
    1.1风控行业背景
当前，经济下行导致中小企业经营成本不断增加吗，产品销售价格因结构原因和市场原因相对走低，企业利润空间被进一步压缩，许多中小企业陷入经营困境，导致企业经营风险加大、连锁性风险陡增、潜在信用风险上升、企业主的道德风险聚升。一些重点领域的银行等金融机构信贷风险进入了一个暴露期，一些地区的金融机构已经出现不良贷款回升苗头，不良贷款高危行业中，钢铁与建材等行业信用风险快速上升，制造业领域新增的不良资产已占到整体不良资产的七成以上，与此同时经济下行也使得个人信贷中的逾期率陡增，不良贷款率上升，如何防控信贷风险，已成为商业银行等金融机构扼待解决的课题。
1.2国内外风控技术现状
序数衡量法：只能反映企业间信用风险的高低顺序，如BBB级高于BB级，但其间的级差无法进行客观量化。
Creditmetrics;Credit Risk+Credit PortofolioView+:是组合投资的分析方法，注重直接分析企业间信用状况变化的相关关系，但是它局限于投资组合分析。
KMV：从单个授信企业在股票市场上的价格变化信息入手，着重分析该企业体现在股价变化信息中的自身信用状况，但对企业信用变化的想关心没有给予足够的分析。
FICO：FICO在方法上通常采取逻辑回归和决策树。然而，这两类方法是存在很大缺陷的。例如，逻辑回归一般只能包含至多10-15个风险因子，且各变量必须服从正态分布；决策树要求对所有申请者的分类是完全互斥的。显然，这些要求是难以满足的，由此产生的结果是“偏误”还是“错误”也很难评价。
ZestFinance: ZestFinance包含70000个变量，利用10个预测分析模型进行集成学习或者多角度学习，并得到最终的消费者信用评分。然而，ZestFinance进行信用评估时，传统征信数据要占到至少30%清晰的用户定位，完善的征信体系支撑，是ZestFinance在美国生存的土壤。中国没有集中的征信所，金融体系也尚不完善。很难适应中国目前的信贷业务。
国内大部分中小银行信用风险管理仍停留依靠经验判断传统阶段，以感觉、经验、关系决策；缺乏对客户信用评级、统一授信风险量化信息系统；缺乏对公司类客户、个人客户优劣的判别统一标准【缺乏对客户风险量及授信边界系统科学的模型。
1.3风控行业发展趋势
随着近年来国 内 大数据互联网 金融的蓬勃发展, 顶尖的数据机构开始从事各种信用 维度的数据收集、 分类、 查询服务, 这为在线征信与量化风险提供了 技术、 数据基础。
多维度数据分析、数学建模、机器学习算法、云计算。
二、YUNRISK风控在线介绍
2.1数据来源：华中大数据交易中心、万德数据库、金融界、中国人民银行、世界银行等。
2.2基于大数据进行分析
     多渠道获取数据，对宏观经济，行业数据，企业数据及个人数据进行分析，多角度，全方位进行风险量化。
企业指标：宏观指标，行业指标，企业指标，财务指标
个人指标：宏观，行业，个人。
指标频率：日，月，季，年
技术特点：
物理学的布朗运动理论：分子运动无规则性、永不停歇性、温度越性。
市场是随机波动的，随机波动是市场最根本的特性。变量过去的历史和变量从过去到现在的演变方式则与未来的预测不相关。也就是说一种现价已经包含了所有信息，包括所有过去的价格记录。同时，价格与粒子运动一样，具有“温度”越高，运动越明显的特性。
蒙特卡罗模拟
基于大数据分析，由于涉及指标众多，如何构建少而精的指标体系是一个极为重要的问题。
Peason和Searman相关性检验法
通过适时检测和监控风控指的相关性来优化指标体系。
主成分分析法
主要用于统计和筛选主要相关的因素。
Probit-logit方法
主要致力于分析所选因素的动态变化，预测其运行轨迹。
2.3系统介绍
个人版风控系统查询:
A个人收入
B银行流水
C负债
D汽车折旧系数
E房产折现系数
企业版风控系统查询
绝对指标
A资产总计
B负债总计
C营业总成本/营业总收入
D销售毛利率
现金收益
E净资产收益率ROE
F经营活动净收益/利润总额（TTM）
G经营性现金净流量/营业总收入
H筹资活动产生的现金流量净额占比
I投资活动产生的现金流量净额占比
偿债能力
J资产负债率
K有形资产/总资产
L权益乘数
M流动比率
N速动比率
营运能力
O存货周转率
P应收账款周转率
Q应付账款周转率
R净资产（同比增长率）
S固定资产投资扩张率
T利润总额/息税前利润
U股东权益合计/负债总计
V. EBITDA率%
 
三、风控流程
3.1业务流程
1.借款人进行咨询；
2.填写申请表和有关资料，提交给业务员；
3.业务员添加客户至客户室；
4.业务员为客户发起授信申请，进入授信审核，审核成功后，借款人获得授信额度。
5.业务员为借款人发起借款申请，进入借款审核，审核成功后，财务放款，借款成功.
3.2授信审核流程（贷前流程）
1.业务员为自己客户发起授信申请；
2.业务主管进行初审，审核通过进入风控委员初审，驳回返回上级，拒绝的授信失败；
3.风控委员进行初审；
4.风控主管进行复审；
5.贷审会进行审核；
6.总经理进行终审，审核通过，授信成功，借款人获得授信额度。
 
3.3
借款审核流程（贷中流程）
1.借款人拥有一定的授信额度，业务员为借款人发起借款申请；
2.业务主管进行初审，审核通过进入风控委员初审，驳回返回上级，拒绝的授信失败；
3.风控委员进行初审；
4.风控主管进行复审；
5.总经理进行终审；
6.审核通过的，财务放款，借款人借款成功。
3.4贷后流程
贷款到期，借款人还款。其中借款人可以提前还款，若到期未能还款，则有展期申请、强制结清、押品结清、押品处置、违约金法系处理。
借款人还清贷款，即可拿回抵押物品。
四。风险管理全面解决方案
阅读全文
版权声明：本文为博主原创文章，未经博主允许不得转载。 举报
目前您尚未登录，请 登录 或 注册 后进行评论
相关文章推荐
互联网金融做大数据风控的九种维度

互联网金融做大数据风控的九种维度 在互联网金融迅猛发展的背景下，风险控制问题已然成为行业焦点，基于大数据的风控模型正在成为互联网金融领域的热门战场。那么，大数据风控到底是怎么一回事呢？与...
liberty_xmliberty_xm2016-11-16 10:051559
【我来解惑】.Net应该学什么怎么学（二）

接上篇《【我来解惑】.Net应该学什么怎么学（一）》。二、C#面向对象基础       初学者学面向对象的时候没必要（也做不到）把面向对象学的非常透、非常深，因为如果想深刻的理解面向对象，必须要有大量...
cownewcownew2010-12-01 23:333045
 
《程序员看天下》实战：揭秘携程大数据的应用处理

一直以来，携程拥有海量数据，如何存储、分析和应用这些数据一直是部门痛点所在！携程大数据团队将会给出什么样的解决方案呢？开源产品的选型和运维又该如何抉择呢....
html-js-广告层的关闭与显示案例

吕老师视频-http://www.tudou.com/home/xuexi158    #logo{        position:absolute;     ...
lvzizhengfranklvzizhengfrank2011-04-29 10:49755
四步教你：开发风控模型？

业务定义 　　为什么把业务定义放在最底层呢? 　　从商务智能的角度说，模型，评分，策略等都是为业务服务的，脱离了业务场景的模型和评分是无本之木，无源之水;脱离了业务场景谈模型的准确性，没有...
liberty_xmliberty_xm2016-11-22 10:347237
【金融干货】四步教你：开发风控模型？

“你的模型准么?” 　　“你的模型真的有用么?” 　　“你的模型对风控有价值么?” 　　在为P2P公司建立风控评分模型过程中，这是最常见的问题。为了回答这一问题，我们想先讨论下如何从实际业务出发...
wangwei134816wangwei1348162016-09-03 09:433550
互联网风控模型需要多大数据？

近两年来，“大数据”一词广受热议，提高了企业对数据及数据所产生的价值的重视，整体上推进了我国在各项商务应用和政务应用中数据支持的比重，提升了量化决策和科学决策的水平。然而，在大数据概念提出之前，我们也...
liberty_xmliberty_xm2016-11-22 10:29816
大数据风控的现状、问题及优化路径（总结侯畅、唐时达文章）

大数据风控的现状、问题及优化路径   一、大数据风控发展迅速 （一）国外案例 Zest Finance公司开发10个基于机器学习的分析模型，1万条原始信息，7万个特征变量，5秒内完成。 Ka...
liberty_xmliberty_xm2016-11-16 09:41873
jquery实现城市三级数据联动的实例

这是第一使用Jquery实现城市三级数据联动的实例！之前本人没有使用过Jquery，在WebForm的开发过程中，很多时候用的是ASP.NET Ajax,ajaxPro +JavaScript,从未使...
nowgoantnowgoant2011-05-06 09:58429
Scorecards - AUC与ROC与Gini

From http://beader.me/2013/12/15/auc-roc/ 二元分类器 ??二元分类器是指要输出(预测)的结果只有两种类别的模型。例如预测阳性/...
textboytextboy2015-07-23 23:341816
大数据风控具体是怎样的？

网贷天眼讯：数据风控是目前Fintech领域的主要应用，也是资本关注的焦点。很多互金公司都开发了大数据风控模型，业界也涌现了很多专门做大数据风控技术然后向业界输出技术能力的技术型公司。然而，大数据风控...
liberty_xmliberty_xm2016-11-22 12:062062

大数据分析的案例、方法与挑战

2015-12-22 20:192.14MB
下载

大数据概念、技术、特点、应用与案例

2014-06-16 09:0554KB
下载

杨大江：桉树混合云及大数据平台案例

2014-05-29 14:061.66MB
下载

Facebook大数据实时分析案例分享(Uri).pdf

2015-09-26 11:478.17MB
下载

Facebook大数据实时分析案例分享(Uri).zip

2017-07-15 17:007.70MB
下载

R语言大数据挖掘机器学习统计案例50本语言全集三套之三

2015-05-27 16:468.65MB
下载

大数据高并发架构实战案例分享-ppt资源.rar

2017-03-30 14:2910.28MB
下载

王家林亲授的上海7月6-7日云计算分布式大数据Hadoop深入浅出案例驱动实战报名信息

2013-06-15 10:54120KB
下载

王家林的云计算分布式大数据Hadoop深入浅出案例驱动实战

2013-06-07 22:1527KB
下载

hadoop大数据平台应用案例

2017-04-15 17:443.17MB
下载
 
风控建模-Mr徐

＋关注
原创
3
 
粉丝
0
 
喜欢
0
 
码云
未开通
他的最新文章更多文章
大数据风控具体是怎样的？
风控建模（高级）分析师
互联网金融时代下机器学习与大数据风控系统
互联网金融风控措施

编辑推荐
最热专栏
互联网金融做大数据风控的九种维度
【我来解惑】.Net应该学什么怎么学（二）
html-js-广告层的关闭与显示案例
四步教你：开发风控模型？
【金融干货】四步教你：开发风控模型？
在线课程

自然语言处理在“天猫精灵”的实践应用
自然语言处理在“天猫精灵”的实践应用
讲师：姜飞俊
蚂蜂窝大数据平台架构及Druid引擎实践
蚂蜂窝大数据平台架构及Druid引擎实践
讲师：汪木铃


喜欢
 
收藏
 
评论
 
分享
 
CSDN首页
学院
下载
更多
下载 CSDN APP
写博客
登录|注册
csdn首页移动开发架构云计算/大数据互联网运维数据库前端编程语言研发管理综合全部 
风控评分模型
转载 2016年04月08日 09:59:14 744702
         

一 概述
余额支付的风险识别模型分为两类：（1）盗号交易识别风险 和 （2）盗卡交易识别风险。其中盗卡交易识别风险和余额有关主要是由于骗子注册号码帮盗来的卡，然后进行充值到余额，通过余额支付销赃。（1）和（2）两种针对的情景不一样，采用的特征变量和变量的重要性很大程度是不一样的。针对（1）的问题，主要是看当前交易相对用户之前的行为是否存在异常。针对（2）的问题，主要看用户信息和绑卡的信息匹配的一致性，可信性，以及当前账号的可信度。
在整体篇，提到风险识别领域采用的常规的方法是专家规则系列和模型系列。规则体系中每个规则针更多对单一风险场景和问题来制定的，偏重精准性和稍微兼顾覆盖率。模型系列更加覆盖率，模型不断学习来增加识别各种风险场景的能力。模型的一个好处就是可以不断学习，对各种风险场景可以有个统一的量化评估，比如0-100分。如果一个风控团队想对外输出风控能力，一个必备的能力，就是必须对外输出风险评分，决策层让客户自己做，而不是直接输出拒绝，还是放过。这篇，我主要谈谈关于盗号的风控模型：余额支付盗号交易识别的风险评分模型。主要围绕图1展开：


二 样本和特征
风险评分模型可以看成一个二分类问题，就是设计个模型能把好的交易样本和坏的交易样本尽可能区分出来。做风险评分模型这个项目前，先得积累足够多的数据（样本和特征），不然真是巧妇难为无米之炊。所以，系统需要有套收集数据的机制，尤其是坏样本的数据收集机制。对于交易而言，可以以订单号来标记一条样本，样本由多个特征变量组成，这些特征变量基本可以包含交易维度的变量，交易双方的特征变量等。首先，系统需记录整体交易这些相关的数据。然后，通过人工标记坏样本的方式来记录坏样本订单号，在支付领域坏样本人工标记方式可以通过用户报损反馈，也可以是人工通过相关黑信息关联找出来的标记样本。系统设计是尽可能多的和并且尽可能精准的的收集到坏样本。对于好样本，如果样本特征变量中不包含某些周期性变量，可以负采样过去几天的交易样本，最好有距离目前时间一周以上的时间间隔，方便用户反馈，从里面剔除坏样本和某些设定规则下的过滤的样本（存在异常样本和没有报损的样本）。
在风控建模领域，一个典型的问题就是样本有偏。举个例子说明：假如你发现骗子符合某些聚集特征，你指定策略1进行打击后，骗子的这种欺诈手段被控制，以后的损失案例都不具备这样的聚集特征。如果你的坏样本的收集时间在策略1上线之后，这个时候模型训练的结果极有可能出现满足聚集特征的风险低，不满足聚集特征的交易反而风险高，也就是说聚集特征的权重是负数。这时候模型的解释性出了问题，这个也是模型训练中一个过拟合问题的范畴。为了有效解决这个问题，可以根据业务经验来查看模型中变量的权重是否与经验相悖，如果相悖，需要仔细评估。对于是样本有偏带来的问题，可以通过重新加入符合某些条件的样本来弥补。对于这些弥补的样本获取方法一种可以从拦截样本中选择，一个可以根据经验来人工生成样本。
谈谈模型的不平衡学习。风控模型学习是个典型的不平衡学习问题，他同时具备不平衡学习领域两个问题：（1）正负样本比率悬殊，但是正负类样本都足够多；（2）正样本样本个数也很稀少。第一个问题是基本满足样本在特征空间的覆盖情况，只是比率较大导致某些学习模型应用会出现问题。第二个问题是样本太少，导致样本在特征空间的覆盖很小，极容易过拟合，不能覆盖特征空间和对欺诈场景的覆盖。对于第二个问题，最好的方法还是先收集样本+一些不平衡学习方法。对于正负样本的比率问题，有的用1:1，有的人用1:10，有的说是1:13.这些大多都是经验。我一般用，其实也是经验，1:10。其实，对于比率这个问题，说到底就是负样本该采样多少的问题。我觉得只要保证负样本也尽可能多满足覆盖特征空间就好，因为很多负样本（好的交易样本）模式都是很相似的，对于相似的模式不用保留太多的样本。但是本来正样本就少，如果负样本和正样本一样多，我个人认为随机采样的负样本覆盖的特征空间会很小，所以，我个人不是很赞同1:1的比率。具体可以参考我的这篇博客：http://blog.csdn.net/hero_fantao/article/details/35784773
三 特征预处理
特征大体可以分为连续特征变量和类目特征变量。特征预处理主要会围绕这两类特征来进行的。主要分为缺失值填充，异常值处理，连续特征归一化处理，连续特征离散化处理。
3.1 缺失值填充
特征的缺失值填充前，我们需要先统计特征的缺失值比率。采用某个特征来区别正常交易和异常交易前，这个特征的缺失值比率不能超过一定的阈值。对于缺失值填充的常用方法有：均值，中值，0值等。
3.2 异常值处理
可能由于某些原因，导致系统在收集样本时候，出现错误，特征值过大或者过小。当然，这个可能本来数据就是这样，但是，我们也需要做个处理。常用的方法：设置分位点做截断，比如0.1%,99.9%分位点等。
3.3 连续特征归一化处理
对于连续特征，比如用户的注册时间间隔，原来的值范围各自不同，不在统一的尺度。有的连续特征值范围大，有的连续特征值范围小。如果不做归一化处理，连续特征中值范围的大的特征会淹没值范围小的连续特征对模型的影响。所以，有必要对连续特征做归一化处理。
常用的连续特征归一化处理方法：（1）min-max方法； （2）z-score方法。
对于互联网数据，很多特征呈现长尾power-law分布，所以，大多场景针对这种情况在做min-max 或者z-score之前，会对连续特征先做log(x)变换。
3.4 连续特征离散化处理
相对连续特征归一化处理，还可以对连续特征进行离散化处理。在logistic regression中，大家经常会把连续特征做离散化处理，好处：（1）是避免特征因为和目标值非线性关系带来的影响；（2）离散化也是种给lr线性模型带来非线性的一种方法；（3）方便引入交叉特征；（4）工程实现上的trick。
常见的离散化处理手法：非监督的方法和监督的方法。非监督的方法：等宽，等频，经验，分布图划分等。监督方法：基于信息增益或卡方检验的区间分裂算法和基于信息增益或卡方检验的区间合并算法等。我个人常用的监督的方法是合并算法，其中具体的介绍可以参考我的这篇博客：http://blog.csdn.net/hero_fantao/article/details/34533533
在风控采用lr模型的时候，对于连续特征采用离散化处理会有个这样的问题：因为我们的坏样本是针对过去的欺诈场景的，欺诈手法在长期博弈中不断升级。我们不仅要让模型尽可能多的覆盖过去的欺诈手法，对未来产生欺诈对抗有一定的适应性，不至于失效太快。采用离散化处理后，就可能出现很大的跳变性。假设我们过去的的坏样本都是刚注册不久的用户，那注册时间间隔做离散化处理时候，就可能分为A，B两段，离散化处理后可以看成0-1二值变量，落在A段为1，否则为0。 为1时候风险高，权重为正值。如果这个变量在过去对正负样本区分度很高，可以看成核心变量的话，那如果骗子绕过A段，跳到B段的话，对模型的预测能力衰弱会是致命的。
四 特征选择
模型训练前必不可少的一项工作就是特征选择，包括特征重要性和决定最终哪些特征会进入模型。对于一个领域专家来说，看你采用的特征集合和以及特征的重要性分布基本就能看出你模型大体会对那些场景预测的准，哪些场景你是预测不出来的。在风控领域就相当于特征集合决定你能覆盖哪些欺诈场景，会对哪些场景的正常交易进行了误判。对于一个风控领域新人来说，最快的进入领域就是看目前风控系统模型采用了哪些特征集合以及特征的重要程度。
谈谈在模型训练前做特征选择的几个好处：（1）去除冗余，不相关特征；（2）减少维度灾难；（3）节省工程空间成本。常用的方法：（1）信息值：information value，简称IV值；IV值越大，重要程度越高。（2）信息增益： information gain； 是采用信息熵的方法，信息增益表示信息熵的变化， 增益越大，说明特征区分度越明显。（3）前向后向选择，依赖模型，通过AIC或者BIC来选择最优特征集合。
五 模型
    5.1 模型简介
这里采用的模型是logistic regression ，简称LR模型。选择这个模型的理由：（1）简单，可解释性强；（2）线上实施响应时间快，风控有在线实时响应时间限制，所以在特征变量使用和模型复杂度上都有要求。
特征变量方面：基于历史的变量需要提前计算好，调用外部接口所需要的变量要么在支付环节之前某个环节预获取或者采用异步方式（异步方式会影响当前判断的准确性）。
模型方面：最好选择简单和泛化能力强的模型，复杂或者ensemble model在离线实验也许表现好，但是在线上未必好，复杂模型尤其是GBDT这种ensemble模型在风控数据下容易过拟合（风控数据小）。从我在风控应用模型的经验来看，目前阶段还不是拼模型的阶段，更多是找到风险特征。模型对恶意行为识别不好，更多可能是恶意特征没覆盖或者突破了当前模型的几个核心变量。
下图是LR模型的简介: 

这是个预测函数，训练样本就是为了求解这个w。这里面涉及损失函数设计问题和最优值求解问题。常用的损失函数是logloss：

模型中采用正则化是为了避免过拟合，我觉得风控建模上一个重要问题就是过拟合，避免几个核心变量的权重过大。常用的最优求解方法有如下几种：（1）batch 梯度下降法；（2）L-BFGS。具体细节可以参考：
http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
5.2 模型训练和评估
训练： 划分数据集为训练集和测试集： 采用 k-fold cross-validation 交叉验证。K可取5或者10等。选择模型，如Logistic Regression 模型，调节参数，对训练集进行训练，直至模型收敛，然后对测试集进行预测。可以用k-fold的平均结果作为整体预测结果来衡量模型。
评估指标： AUC，准确率和召回率, F1-score等。
   下图是ROC曲线和风险评分预测分值的累积分布：

   这里面再提一点：就是上面这些评估指标即使表现良好，但是也未必说明模型应用没什么问题。常见的一点：特征的相关性影响（特征相关性对模型抗噪性有影响）。对于强相关的特征需要做下处理，能整合成一个变量最好。特征相关性在模型结果的表现上可能会出现特征的权重正负方向和大家认知相反，比如某个高风险特征和预测结果应该呈现正相关，但是模型结果显示却呈现负相关。这个大多由于另外一个更强特征和该特征呈现相关性造成的。相关性导致的这些问题，会让模型的解释性出现问题。在风控领域，模型解释性很重要。
5.3 风险评分的应用
计算线上不同分值段交易量大小，最好能给出不同分值段恶意交易比率。可以根据不同业务场景设定不同分数阈值，即使同一场景也可以根据不同分数阈值来进行不同的惩罚手段，分数很高的时候可以进行冻结账号等。值得一提的是，交易行为中有一定数量的高危行为，但是这些高危行为未必都是欺诈行为，异常不代表欺诈。很多正常的人某些行为和欺诈很相似，同样欺诈者随着博弈对抗加剧，也越来越伪装成正常交易。在风控，有时候为了增加对欺诈行为的覆盖，牺牲一小部分用户支付体验，也是值得的。我认为，风控一个重要的工作就是在风险和支付体验上获取平衡，如果支付体验不好，风险控制再好，也是没有意义的。
风险评分应用一个重要的方面：对交易评分实时查询，相应变量值展示，以及重要变量触犯展示等一些列解释行为。这块叫做告诉别人为什么你风险高或者为什么风险低。
阅读全文
举报
目前您尚未登录，请 登录 或 注册 后进行评论
相关文章推荐
余额支付风控--整体篇

余额支付风控--整体篇 什么是风控？在支付行业的的风控是通过一定手段来对平台的业务风险进行控制，偏业务安全领域。风控和技术安全还是不太一样，技术安全更多关注系统漏洞，是否被攻击，是否被拖库。 所以，...
hero_fantaohero_fantao2015-03-07 22:082172
《Credit Risk Scorecard》 第六章: Scorecard Development

第六章：Scorecard Development Process, Stage 4: Scorecard Development 开发流程： 对于申请评分卡（A 卡）来说，下面是整个开发流程。...
hero_fantaohero_fantao2017-03-31 17:07583
 
零基础的学习心路：12个机器学习案例实战！

都说今年是AI开发元年，为了转型AI技术程序员，这小半年来看了几本书，总结了一些学习的方法和踩过的坑儿，今天我想谈谈关于机器学习该如何入门以及学习方法....
余额支付风控 -- 风控评分模型篇

余额支付风控                        ...
hero_fantaohero_fantao2015-03-07 22:202974
4步教你开发风控评分模型

“你的模型准么？” “你的模型真的有用么？” “你的模型对风控有价值么？” 在为P2P公司建立风控评分模型过程中，这是最常见的问题。为了回答这一问题，我们想先讨论下如何从实际业务出发...
mousevermousever2016-01-05 11:382976
支付平台风控系统架构模型设计

下面是一个支付平台异步风控的模式。开发相关类似系统的人可以参考下。
wangyangzhizhouwangyangzhizhou2014-03-15 15:235310
风控系统架构设计

风控系统架构设计。
wangyangzhizhouwangyangzhizhou2014-03-17 14:304793
四步教你：开发风控模型？

业务定义 　　为什么把业务定义放在最底层呢? 　　从商务智能的角度说，模型，评分，策略等都是为业务服务的，脱离了业务场景的模型和评分是无本之木，无源之水;脱离了业务场景谈模型的准确性，没有...
liberty_xmliberty_xm2016-11-22 10:347237
Scorecard 评分卡模型

公式 woe=ln(odds)，beita为回归系数，altha为截距，n为变量个数，offset为偏移量（视风险偏好而定），比例因子factor。   总评分。或去掉负...
textboytextboy2015-07-20 23:1914004
R语言-评分卡模型验证（ROC,KS，AIC,BIC)

本文主要记录几种常用的模型检验方法：ROC、AUC、Gini系数，KS曲线。重点在R语言的使用上，暂时不包括检验方法的原理。博主刚开始使用R语言不久，因此也借此机会整理记录自己的学习过程。如有不当，欢...
KIDxuKIDxu2017-05-31 14:501837
WOE信用评分卡--R语言实例

目录(?)[-] 信用卡评分一数据准备二数据处理三变量分析四切分数据集五Logistic回归六WOE转换七评分卡的创建和实施 转载自：http://blog.csdn.net/cs...
qq_16365849qq_163658492017-03-28 10:411476
4步教你开发风控评分模型

2015-08-12 刘时斌 数信互融 作者：刘时斌（数信互融研发负责人、联合创始人） 企业公众号：数信互融 “你的模型准么？” “你的模型真的...
wa2003wa20032015-11-10 08:141848
金融风控-->申请评分卡模型-->特征工程（特征分箱，WOE编码）

这篇博文主要讲在申请评分卡模型中常用的一些特征工程方法，申请评分卡模型最多的还是logsitic模型。先看数据，我们现在有三张表：已加工成型的信息：Master表 idx:每一笔贷款的unique ...
Mr_tytingMr_tyting2017-07-16 21:26992
金融风控-->申请评分卡模型-->logisticRegression建模

上一篇博文中，我们对数据进行了特征工程处理，包括特征分箱，WOE编码，计算IV值，进行单变量，多变量的分析等一系列的处理。总结特征分箱的一些处理办法如下图的流程图：那么本篇博文在上篇博文的基础上，建立...
Mr_tytingMr_tyting2017-07-23 10:48851
金融风控-->申请评分卡模型-->申请评分卡介绍

从这篇博文开始，我将总结金融风控中的另外一个模型：申请评分卡模型。这篇博文将主要来介绍申请评分卡的一些基本概念。本篇博文将以以下四个主题来进行介绍说明： 信用风险和评分卡模型的基本概念 申请评分卡在互...
Mr_tytingMr_tyting2017-07-14 10:521036

客户经理素质自我评分模型

2011-02-24 10:58142KB
下载

澳新银行评分卡模型建立流程

2017-09-08 14:5493KB
下载
评分法模型开发-WOE值计算

对入模的定量和定性指标，分别进行连续变量分段（对定量指标进行分段），以便于计算定量指标的WOE和对离散变量进行必要的降维。对连续变量的分段方法通常分为等距分段和最优分段两种方法。等距分段是指将连续变量...
lll1528238733lll15282387332017-08-03 00:12486

竞赛评分公平性的模型

2009-05-23 14:37524KB
下载
评分卡模型开发-定量指标筛选

模型开发的前三步主要讲的是数据处理的方法，从第四步开始我们将逐步讲述模型开发的方法。在进行模型开发时，并非我们收集的每个指标都会用作模型开发，而是需要从收集的所有指标中筛选出对违约状态影响最大的指标，...
lll1528238733lll15282387332017-08-02 23:40569
基于改进的协同过滤算法的用户评分预测模型

目标：精确预测某目标用户对目标电影M的评分值。 步骤：1得到与目标电影M最相似的K个电影集合ci。 2.基于ci,计算评论过m电影且与目标用户最相似的k2个用户 3．将与目标用户最相似的k2个用户对目...
huozi07huozi072015-05-09 14:011011
 
Michael_Shentu

＋关注
原创
139
 
粉丝
0
 
喜欢
0
 
码云
未开通
他的最新文章更多文章
XGBoost解决多分类问题
ROC AUC的原理详解
先验概率与后验概率、贝叶斯区别与联系
一个 Q-learning 算法的简明教程

编辑推荐
最热专栏
余额支付风控--整体篇
《Credit Risk Scorecard》 第六章: Scorecard Development
余额支付风控 -- 风控评分模型篇
4步教你开发风控评分模型
支付平台风控系统架构模型设计
在线课程

自然语言处理在“天猫精灵”的实践应用
自然语言处理在“天猫精灵”的实践应用
讲师：姜飞俊
蚂蜂窝大数据平台架构及Druid引擎实践
蚂蜂窝大数据平台架构及Druid引擎实践
讲师：汪木铃


目录
 
喜欢
 
收藏
 
评论
 
分享
返回顶部


登录 | 注册
关闭

creditx的博客
 目录视图 摘要视图 订阅
异步赠书：9月重磅新书升级，本本经典           程序员9月书讯      每周荐书：ES6、虚拟现实、物联网（评论送书）
 浅谈大数据风控的基本框架
标签： 互联网金融大数据风控消费金融人工智能
2017-05-26 13:38 503人阅读 评论(0) 收藏 举报
目录(?)[+]
本文转载自 CreditX氪信，作者 唐正阳
 
近日，中国人民银行成立金融科技（FinTech）委员会，旨在加强金融科技工作的研究规划和统筹协调。
 
随着AI、云计算在金融业务层面的快速渗透，也倒逼监管跟进升级，以进一步加强监管的有效性。事实上，这次央行提出监管科技（RegTech），也是对金融科技的肯定，希望其在驱动金融创新，引领金融规范化发展中发挥积极作用。
 
金融的核心在于风险，现下谈及互联网金融，很多人都对大数据风控并不陌生，也都在行业野蛮发展的过程中有自己的理解。但如果要定义什么是大数据风控，可能不少行业外围同学的看法难免有些偏颇和碎片化。下面笔者浅显地从大数据风控的基本内容和框架出发，主要为想要了解这个行业的同学简要做一个相对完整的介绍。
 
大数据到底有哪些？
首先我们需要厘清大数据的概念，当下各行各业都在探索大数据的价值，大数据的定义也很多。从技术的角度来理解，本质就是来自多个渠道和系统的结构化和非结构化数据，在金融领域，尤其是消费金融，我们界定大数据到底有哪些维度，其实芝麻信用分是一个非常典型例子，虽然不同公司有自己的分法，但数据主体大都可以归类为身份属性、信用属性、行为数据、消费属性、人脉关系这五大方面。
 

 
身份属性，这是最基础的，包括真实的身份信息、学历、就业经历等。
 
信用属性包含的方面比较多，比如过往的履约记录、固定资产、流动资产、收入等都会纳入进来用以衡量一个人还款能力和还款意愿。
 
过去我们去银行办理贷款，以上这两个维度就是传统风控的数据来源，但由于大多数人没有这方面比较完整的记录，且流程冗长麻烦，因此只有少部分人才能享受金融服务。
 
现在随着互联网的爆发式发展，以及普惠金融的崛起，越来越多的机构正将海量互联网数据和金融结合探索其在表征风险方面的价值。如上所述，长期缺乏金融产品的人群基数庞大，需求旺盛，因此从创业公司到BAT到金融机构，都在拓展更多领域的大数据以抢占消费金融爆发的窗口期。
 
数据拓展的第一个领域是消费属性，这块主要是电商或交易数据，比如日常购物商品、消费金额、消费时段等都可以从不同角度来分析出一个人消费稳定度，消费档次，还款能力等风险特征。
 
其次，人脉关系也是很重要的一个维度，俗话说“近朱者赤，近墨者黑”。很多时候你的微信朋友圈、经常打电话的朋友也反映了你在风险上的表征。生活中我们和同事沟通比较频繁，而如果一个人社交稳定度差，可能说明他经常换工作，显然风险也会相应增加。
 
最后一个维度是行为数据，这块数据涉及面比较广泛，主要是用户在APP上的活动所体现出的行为特点，包括浏览不同类目的频次、时间、风险偏好等等。
 
说到这，我们对大数据风控涉及的几类数据应该有了一个基本的认识，那在具体工作中，我们是不是应该先把这些数据都收集好才能做好工作呢？
 
答案是否定的，因为数据都是有成本的，开展一个金融业务，从数据、模型、服务再到最后的收益，更为关键的是业务本身的形态，再进一步扩展至产品、场景，我们会发现不同的场景面对不同的客群，风控关注的风险点都会有一定的差异，再反过来寻找能够表征这些风险点的数据也会有所侧重。因此，如果要厘清大数据风控该用哪些数据，首先要对风控场景有所了解，其中最为核心的是理解不同场景下要抓住哪些金融风险。
 
金融风险的理解
 
风险的概念比较大，为了给大家提供一个最基本的视角来了解，下面以线上信用贷款举例来阐述，这也是互联网消费金融最主要的方向之一。在这个场景下，我们面临的风险主要是信用风险和欺诈风险两块。
 
首先是欺诈风险，据数字，在中国，互联网金融50-70%的损失来自欺诈，这也可以说是风控业务中最困难的地方。造成这种现状的原因比较多，一方面是诸如现金贷类型的消费金融短期爆发式发展，大量创业公司涌入赛道以互联网获客运营的流量思路做金融，与此同时相应的风控经验和能力缺失，因此给专业的欺诈分子暴露了较大的风险敞口，通过简单研究业务规则漏洞，并通过互联网传播，可能带来较大的损失；另一方面也是欺诈产业链自身研究实力不断的完善有关，现在的欺诈已经从以往单一的个人欺诈演变为有组织、有规模的集团化欺诈，链条上盗号，数据泄露作为基础账号库，已衍生出一系列包括黑产交易、ID Mapping、定向攻击的完整产业链，其中的分工和技术也非常专业和精细化。
 

 
如图：这种设备称为养卡设备，俗称“猫池”，实际上就是一个号码卡插槽，可以在不拆卡的情况下将整张卡插到猫池里，连接电脑后使用，还可以接收短信验证码。现在在各种平台注册时都需要填写短信验证码，“刷手”为了获取平台的福利会利用猫池养卡，规避平台的身份确认短信。
 

 
再比如短信拦截马，这是一种可以拦截他人短信的木马，让被攻击者收不到短信，并将短信内容截取到攻击者手机上。这种木马最常见的是通过钓鱼、诱骗、欺诈等方式诱导用户安装，然后通过拦截转发用户短信内容，以此获取各种用户重要的个人隐私信息，如用户姓名、身份证号码、银行卡账户、支付密码及各种登录账号和密码等，造成这些信息的泄露，再利用此信息从而达到窃取用户资金的目的，严重威胁用户的财产安全。
 
第二块简单谈一下信用风险，其定义是借款者违约的风险，换句话说，也就是借款人因各种原因未能及时、足额偿还债务或贷款而违约的可能性。一般，我们会从还款能力和还款意愿两个角度去分析信用风险，但在小额信用贷场景中，由于额度一般为2000左右，少就几百，多也就5000，一个正常有工作的人很少会不具备按期还款的能力。所以在这点上我们更多的是从还款意愿角度来看，即借款人对偿还贷款的态度，现实中有不少人会借钱不还，这就是常说的“老赖”，如果我们以违约概率的目标去识别他，还是能挑出不少有正常借款意图的人。
 
大数据风控体系的初步构建
 
现在我们有了对大数据和风险的基本认知，但如何真正从大数据提炼出风险表征，并进一步转化为实时的金融风险决策服务呢？事实上，这需要重构一整套风控数据架构体系，过去传统金融机构在身份属性和信用属性的数据上沉淀了丰富知识，但在互联网金融业务中，用户能够关联的更多是消费、社交和行为类数据，且越是小额分散的业务，数据的金融属性越弱。近两年小额现金贷也正是由于这部分数据的风控知识体系缺乏，因此陷入高利率覆盖高坏账、暴力催收等乱象。
 
随着监管趋严和行业愈加规范化，大数据风控，尤其是基于弱数据的风控正成为线上信用贷业务最重要的核心竞争力，除了基本的身份验证、合规、黑灰名单、规则过滤以外，要防控欺诈风险和信用风险，还需做好以下三个维度的准备：
 
第一是设备层面，现在成熟的APP都需要辟如更换登录设备时重新输入短信验证码，或者登录时得手动滑动验证码等，这些既是挡住黑产的第一道关，也是后续风控的重要数据基础。
 
第二是知识体系层面，拆解开来看，核心有2点，一是知识工程，二是模型。目前最领先的消金机构都有一套相对成熟的针对特定数据domain的风险特征库和分客群、分目标的模型，比如反欺诈模型、申请评分模型、风险行为预测模型等，贷后还会有催收模型、客户流失预测模型等。在这过程中，引入AI处理弱数据，并在大量样本上不断迭代模型是关键。
 
第三是系统层面，试想我们有了清晰的数据认知，结合到场景和风险理解我们也摸索了一套经验证的数据使用方式，但如何与我们的业务系统对接，成为实时的数据服务？这还需要一个完整的支持数据接入、加工处理、得出结果以及监控管理的在线引擎。随着线上个人贷款规模的爆发式增长，控制风险、解放人力已成为最紧迫的需求，数据智能自动化引擎是机构“跑起来”的强大推动力。
 
风控与征信
最后，简单谈一下大数据风控和征信的区别，不少同学会认为风控公司就是征信公司，这其中还是有较大区别的，尤其在大数据领域。
 
据百科，征信是专业化的、独立的第三方机构为个人或企业建立信用档案，依法采集、客观记录其信用信息，并依法对外提供信用信息服务的一种活动，它为专业化的授信机构提供了信用信息共享的平台。
 
应该看到，征信所对应的数据还局限在开头所说的身份属性和信用属性范围，也就是强金融数据，而大数据风控认为所有数据都是风险数据，更侧重于将弱数据金融化，再提炼出风险表征。因此，征信本质是大数据风控的子集，覆盖的人群和应用领域也更为狭窄。当下大数据风控和征信的确还处于混业经营的状态，但随着行业发展，可预见市场还会逐渐细分，届时前者将更注重在数据生态体系上的经验和沉淀，后者则侧重在独立性和公信力以及监管合规等方面。
顶
0
 
踩
0
 
 
上一篇41万亿元规模的消金行业，重构势在必行，AI算法会是突破口？
  相关文章推荐
? 大数据风控具体是怎样的？
? 自然语言处理在“天猫精灵”的实践应用--姜飞俊
? 大数据风控案例（总结他人）
? 蚂蜂窝大数据平台架构及Druid引擎实践--汪木铃
? 浅谈大数据风控的基本框架
? Retrofit 从入门封装到源码解析
? 浅谈大数据框架调用过程中用到的RPC
? 程序员如何转型AI工程师
? 浅谈大数据存储最容易出现哪些问题
? 深入探究Linux/VxWorks的设备树
? 浅谈使用ArcPy执行大数据量处理任务
? 使用QEMU搭建u-boot+Linux+NFS嵌入式开发环境
? 互联网大数据应用：浅谈用户行为分析
? 【转载】〖ASP.NET〗ASP.NET的五大数据控件浅谈
? 浅谈云计算和大数据技术
? 浅谈大数据（hadoop）和移动开发（Android、iOS）开发前景

查看评论

  暂无评论

您还没有登录,请[登录]或[注册]
* 以上用户言论只代表其个人观点，不代表CSDN网站的观点或立场
个人资料
访问我的空间  
CreditX氪信
 
访问：1375次
积分：33
等级： 
排名：千里之外
原创：1篇转载：5篇译文：0篇评论：1条
文章搜索

 搜索
文章分类
新闻(3)
文章存档
2017年05月(2)
2017年03月(1)
2017年01月(1)
2016年12月(2)
阅读排行
浅谈大数据风控的基本框架(503)
氪信CEO朱明杰：真AI只能用钱表达(380)
Michael Jordan、BAT、CreditX探讨大数据技术的前沿进展和实践应用(181)
为什么说集成学习模型是金融风控新的杀手锏？(127)
AI入侵金融？到一线去！AI是来解决实际问题的(107)
41万亿元规模的消金行业，重构势在必行，AI算法会是突破口？(69)
评论排行
为什么说集成学习模型是金融风控新的杀手锏？(1)
Michael Jordan、BAT、CreditX探讨大数据技术的前沿进展和实践应用(0)
氪信CEO朱明杰：真AI只能用钱表达(0)
AI入侵金融？到一线去！AI是来解决实际问题的(0)
41万亿元规模的消金行业，重构势在必行，AI算法会是突破口？(0)
浅谈大数据风控的基本框架(0)
推荐文章
* CSDN新版博客feed流内测用户征集令
* Android检查更新下载安装
* 动手打造史上最简单的 Recycleview 侧滑菜单
* TCP网络通讯如何解决分包粘包问题
* SDCC 2017之大数据技术实战线上峰会
* 快速集成一个视频直播功能
最新评论
为什么说集成学习模型是金融风控新的杀手锏？
king_eagle2015: 关于数据方面的应用，还真是让人头疼呢


公司简介|招贤纳士|广告服务|联系方式|版权声明|法律顾问|问题报告|合作伙伴|论坛反馈
网站客服杂志客服微博客服webmaster@csdn.net400-660-0108|北京创新乐知信息技术有限公司 版权所有|江苏知之为计算机有限公司|江苏乐知网络技术有限公司
京 ICP 证 09002463 号|Copyright ? 1999-2017, CSDN.NET, All Rights Reserved GongshangLogo







CSDN首页
学院
下载
更多
下载 CSDN APP
写博客
登录|注册
csdn首页移动开发架构云计算/大数据互联网运维数据库前端编程语言研发管理综合全部 
金融行业风控概念及分析
翻译 2016年12月06日 15:59:54 122200
互联网金融是指以依托于支付、云计算、社交网络已及搜索引擎等互联网工具，实现资金融通、支付和信息中介等业务的一种新兴金融。做好互联网金融，要立足于三个基本点:平台、数据、金融。而在这其中，大数据，作为连接平台、用户、金融等方面的工具，有着举足轻重的意义。
　　由于互联网金融涉及广泛、囊括多个领域，各领域的风控策略也不尽相同，不能一概而论，下面就大数据风控在互联网金融领域的运用做一个大致的分类和解析。

　　首先，如何理解大数据风控
　　大数据风控的有效性除了强调数据的海量外，更重要的在于用于风控的数据的广度和深度。其中：
数据的广度：指用于风控的数据源多样化，任何互联网金融企业并不能指望依据单一的海量数据就解决风控问题，正如在传统金融风控中强调的“交叉验证”的原则一样，应当通过多样化的数据来交叉验证风险模型。互联网金融的风控策略也如此，可能对同一风险事件采用了多种策略。 
数据的深度：指用于风控的数据应当基于某个垂直领域真实业务场景及过程完整记录，从而保证数据能够还原真实的业务过程逻辑。例如，很多第三方支付平台有丰富的真实交易记录，但由于大部分场景下无法获取交易商品的详细信息及用户身份，在用于风控时候价值大打折扣，因而数据的完整性和垂直深度很重要。

互联网金融产品如何利用大数据做风控，大致有以下一些分类和方向：  
　　1、基于某类特定目标人群、特定行业、商圈等做风控。由于针对特定人员、行业、商圈等垂直目标做深耕，较为容易建对应的风险点及风控策略。
　　例如： 针对大学生的消费贷，主要针对大学生人群的特征
       针对农业机具行业的融资担保。 
       针对批发市场商圈的信贷。

　　2、基于自有平台身份数据、历史交易数据、支付数据、信用数据、行为数据、黑名单/白名单等数据做风控。
　　>>>>身份数据：实名认证信息（姓名、身份证号、手机号、银行卡、单位、职位）、行业、家庭住址、单位地址、关系圈等等。
　　>>>>交易数据/支付数据：例如B2C/B2B/C2C电商平台的交易数据，P2P平台的借款、投资的交易数据等。
　　>>>>信用数据搜索：例如P2P平台借款、还款等行为累积形成的信用数据，电商平台根据交易行为形成的信用数据及信用分（京东白条、支付宝花呗），SNS平台的信用数据。
　　>>>>行为数据：例如电商的购买行为、互动行为、实名认证行为（例如类似新浪微博单位认证及好友认证）、修改资料（例如修改家庭及单位住址，通过更换频率来确认职业稳定性）。
　　>>>>黑名单/白名单：信用卡黑名单、账户白名单等。

　　3、基于第三方平台服务及数据做风控 互联网征信平台（非人行征信）、行业联盟共享数据（例如小贷联盟、P2P联盟） FICO服务、Retail Decisions(ReD)、Maxmind服务。

　　>>>>IP地址库、代理服务器、盗卡/伪卡数据库、恶意网址库等；
　　>>>>舆情监控及趋势、口碑服务。诸如宏观政策、行业趋势及个体案例的分析等等

　　4、基于传统行业数据做风控 人行征信、工商、税务、房管、法院、公安、金融机构、车管所、电信、公共事业（水电煤）等传统行业数据。

　　5、线下实地尽职调查数据
　　包括自建风控团队做线下尽职调查模式以及与小贷公司、典当、第三方信用管理公司等传统线下企业合作做风控的模式。线下风控数据也是大数据风控的重要数据来源和手段。
阅读全文
举报
目前您尚未登录，请 登录 或 注册 后进行评论
相关文章推荐
网页爬虫原理及java实现

网络蜘蛛即Web Spider，是一个很形象的名字。把互联网比喻成一个蜘蛛网，那么Spider就是在网上爬来爬去的蜘蛛。网络蜘蛛是通过网页的链接地址来寻找网页，从 网站某一个页面（通常是首页）开始，读...
lijixianglijixianglijixianglijixiang2016-12-02 12:273365
风控系统架构设计

风控系统架构设计。
wangyangzhizhouwangyangzhizhou2014-03-17 14:304794
 
《程序员看天下》实战：揭秘携程大数据的应用处理

一直以来，携程拥有海量数据，如何存储、分析和应用这些数据一直是部门痛点所在！携程大数据团队将会给出什么样的解决方案呢？开源产品的选型和运维又该如何抉择呢....
html-js-广告层的关闭与显示案例

吕老师视频-http://www.tudou.com/home/xuexi158    #logo{        position:absolute;     ...
lvzizhengfranklvzizhengfrank2011-04-29 10:49755
四步教你：开发风控模型？

业务定义 　　为什么把业务定义放在最底层呢? 　　从商务智能的角度说，模型，评分，策略等都是为业务服务的，脱离了业务场景的模型和评分是无本之木，无源之水;脱离了业务场景谈模型的准确性，没有...
liberty_xmliberty_xm2016-11-22 10:347237
量化还是风控？

量化投资的东西也看了一两天了，怎么样？可以解答你第三个问题了吗？ 总的说来，量化投资，应该我自己的那种尝试，不能叫做尝试，我只是用了一个类似于小市值的策略，然后简单止盈，发现回测效果后，就用来实盘，...
strwolfstrwolf2017-03-31 18:40421
P2P风控措施和风控流程

P2P是一种跳过银行间接贷款融资模式的一种在借款人和出借人之间直接发生借贷关系的业务模式，那么理解P2P的风险是什么就很简单，就是借款人不能偿还借款的风险。
u011437229u0114372292016-11-16 21:081235
ROC、AUC、K-S

#标题中的几个指标常用来衡量一个风控模型的优劣。 信贷场景下，某模型将一批客户分为了两类，一类是好客户，一类是坏客户，比如用logistic回归预测客户违约率，概率大于0.5的认为是坏客户，小于...
u010159842u0101598422017-08-11 14:0372
余额支付风控--整体篇

余额支付风控--整体篇 什么是风控？在支付行业的的风控是通过一定手段来对平台的业务风险进行控制，偏业务安全领域。风控和技术安全还是不太一样，技术安全更多关注系统漏洞，是否被攻击，是否被拖库。 所以，...
hero_fantaohero_fantao2015-03-07 22:082172
手环 联系人

什么意思 新奇陈 哪位老大做橡皮筋的盒，带印刷的 我啊 我是电话是 你好！ 【深圳市财旺丝印电子有限公司】 专业丝印各种产品；商标，标牌，铭牌，不干胶，水晶滴胶,手环，标贴，铝标牌，金...
XiaoXiao2011-12-21 19:030
金融行业的BI应用分析

商业智能是一种提高企业智能化的手段，它可以满足企业发展的需要、提高企业的竞争力。同时，对于提高金融行业的风险管理、提升对外服务的质量都能够起到关键性的作用。
yuanziokyuanziok2016-08-11 09:14991
linux服务器架构设计 金融行业CMS系统）第三篇 【思路分析-安全性】

1.服务器硬件安全； 硬件安全包括：质量保证，安装保证。 质量保证：尽量在资金允许的情况下购买正规渠道品牌服务器，不要怕花钱，要知道质量不好的服务器对企业的影响有多大！ 安装保证：在安装过程中要...
machael4147machael41472015-07-17 12:04366
（linux服务器架构设计 金融行业CMS系统）第二篇 【思路分析-服务器高可用性】

b.上次说到服务器不能存在单点，那么这次说下服务能够智能重启是怎么回事； 大家有的可能会碰到有的服务因为配置不当有时候服务进程会假死，这样就会导致网站不可用，一般人的做法就是重启服务器，还有就需要登陆...
machael4147machael41472015-07-16 11:12492
大数据分析在金融行业的应用与趋势

随着人们的生活和行为不断融入互联网，互联网金融犹如一头突然闯入的猛兽，不断冲击着传统银行的地盘。大数据的出现，给了银行们反击的机遇，借助庞大的金融大数据，银行可以实现精准决策和快速反应。大数据分析在金...
comtop2014comtop20142017-03-01 17:38100
P2P互联网金融行业分析

我越来越觉得，想要深入和客观地探讨一个问题，一定要从多个角度去看待这个问题。角度不同，动机不同，观点和结论也有所不同。 1.求职者    这个行业薪水如何、发展前景如何，即和我的职业发展是否匹...
XingKong22starXingKong22star2014-10-21 10:21857
金融行业部分公司待遇汇总

1.阳光财险：研究生，投资研究岗，全年基本工资+奖金+福利＝8万（税前） 2.汇丰银行：Global markets中国大陆地区外汇trader，月薪8000。汇丰的BDP项目起薪8000，18个...
toughhoutoughhou2011-08-20 11:513249
改变金融行业的区块链应用具体体现在哪里？

比特币背后的区块链技术不仅仅会改变金融行业，还会通过互联网应用模式改革其他领域，从合同签署到投票选举都会发生改变。但是这些应用究竟是由哪些成分构成的呢？ Joel Monegro（Joel ...
qq53016353qq530163532016-04-30 08:44118

金融行业服务总线系统（FESB）整体解决方案

2017-05-03 16:214.10MB
下载

软通动力：中国金融行业转型为IT市场带来的机遇与挑战

2012-11-06 13:5012KB
下载
大数据金融行业企业应用几点思考

一、数据挖掘的价值体现 　　任何数据分析或者挖掘的项目都不会直接产生经济价值和意义，分析出的数据结果既不能给企业直接带来一个客户，也不能帮助企业卖出一件产品。数据分析的价值体现在于业务部门根据分...
bluejasonbluejason2014-10-19 14:46516
 
15005153460

＋关注
原创
29
 
粉丝
0
 
喜欢
0
 
码云
未开通
他的最新文章更多文章
hvie UDF函数
hive UDAF函数
org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8031

编辑推荐
最热专栏
网页爬虫原理及java实现
风控系统架构设计
html-js-广告层的关闭与显示案例
四步教你：开发风控模型？
在线课程

自然语言处理在“天猫精灵”的实践应用
自然语言处理在“天猫精灵”的实践应用
讲师：姜飞俊
蚂蜂窝大数据平台架构及Druid引擎实践
蚂蜂窝大数据平台架构及Druid引擎实践
讲师：汪木铃


喜欢
 
收藏
 
评论
 
分享
 

登录 | 注册
关闭

lcyGo的专栏
 目录视图 摘要视图 订阅
异步赠书：9月重磅新书升级，本本经典           程序员9月书讯      每周荐书：ES6、虚拟现实、物联网（评论送书）
 大数据下的电商风控体系――李学庆
2017-09-04 20:51 196人阅读 评论(0) 收藏 举报
 分类： Hadoop（2）  
由51CTO举办的WOT”互联网+”时代大数据技术峰会上，来自京东商城安全管理部经理李学庆做了以《大数据下的电商风控体系》为主题的演讲。本文章是把本次分享的干货亮点整理成文字形式，呈献广大的用户。
【讲师简介】
大数据下的电商风控体系――李学庆
李学庆，京东安全方向第一人，早在2011年入职京东商城，并承担公司安全质量提升和自动化测试工作。他是京东安全开发生命周期SDL实践者，在前期带领团队规划和实践了上线安全、生产环境安全、重要项目安全、业务部门安全下沉等相关体系流程工作;他在2013年开始规划和筹建京东安全应急响应中心(JSRC)，到目前为止京东核心白帽子已达到百余人，接报漏洞上千余个，并通过接报漏洞建立内部安全技术提升的机制。他通过多年积累整理出一套不同行业定位安全方向的模型"安全决策蜂窝模型"。
近年来，网络诈骗事件非常多，每当电商促销时期，诈骗事件尤其多。随着网络诈骗手法升级，受害者人均损失金额也大幅上涨。来自猎网平台的最新数据显示，去年该平台共收到全国用户提交的网络诈骗举报24886例，举报总金额1.27亿余元，人均损失5106元。与2014年相比，虽然举报数量只增长了7.96%，但人均损失却增长了146.7%，将近1.5倍。
我们该如何反欺诈呢?
据李老师介绍，电商诈骗主要包含结盗号、刷单、欺诈。如果不能做好这三方面的安全防护工作，就会引发诸多灾难。例如：钓鱼、套现、伪装客服、盗卡、伪卡、洗钱、信息泄露、返点、撞库、劫持等。
出现这些情况之后，我们下一步需要做的事情是什么呢?我们需要一套风控平台!电商风控模型需要具备以下几个基本模块：数据提取――数据分析――数据存储――规则学习――对外服务。
典型的风控模型
数据提取：电商风控平台的数据提取与其他的风控模型是不一样的，它可能需要提取各个地方点的数据，例如你的登录、注册、找回密码、以及用户行为。 其中用户行为是现在较为常用的方式，通常用买点的方式做，关注用户所做的交易，支付情况、参与优惠活动的情况等。
数据分析：做数据的分析，对IP、设备、人机、征信等进行画像。
1、IP画像：针对代理、VPN、网关、服务器、IDC、ADSL、归属地、外国、攻击等内容进行分析。
2、设备画像：针对机器型号、系统版本、手机串号、CPU型号、最高频率、虚拟机、移动距离、并行登录、模拟器等方面进行分析。
3、人机画像：针对用户的键盘敲击、鼠标移动、点击偏好以及触屏压力等情况进行分析。
4、征信画像：针对资产、信用卡、身份证、朋友圈、邮箱等信息进行分析。
数据存储：提到数据存储，需要注意以下三个方面，一是应该如何存储风险数据，二是如何存储原始数据，三是风险存储都有哪些状态。
规则学习：要建立新规则，在加入新规则的同事保证不能影响整局的规则，慢慢让整套系统机器学习运转起来，不影响整个风控模型。对于异常特征与正常特征要尤为注意。
对外服务：怎么去对外服务?一是，直接反馈你的状态。例如QQ号，手机号，邮箱等收到诈骗信息或者钓鱼网站的时候，就可以直接提供给腾讯等公司的风控部门。二是，需要为风险分等级。不同等级的风险有不同的处理方式，所以需要事先定义风险等级。三是，选择合适的接入方式。
最后，李老师为大家列举了风控平台实例，并介绍了三大主流风控框架：
大数据下的电商风控体系――李学庆
顶
0
 
踩
0
 
 
上一篇电商实时交易风控系统
下一篇精彩博客
  相关文章推荐
? Http抓包工具--查尔斯
? 自然语言处理在“天猫精灵”的实践应用--姜飞俊
? Dubbo框架 - 3 - 服务器部署(2)
? 蚂蜂窝大数据平台架构及Druid引擎实践--汪木铃
? [正则]正则表达式系统教程.CHM
? Retrofit 从入门封装到源码解析
? django book 2.0 中文版 飞龙整理
? 程序员如何转型AI工程师
? Qt计算器项目开发源码
? 深入探究Linux/VxWorks的设备树
? 史上最简单的 MySQL 教程（三）「 MySQL 数据库」
? 使用QEMU搭建u-boot+Linux+NFS嵌入式开发环境
? 一、 认识.net Web开发
? SpringMVC中WebDataBinder的应用及原理
? web工作流管理系统开发之一 工作流概念
? 万事开头难 - 第一个Android应用

查看评论

  暂无评论

您还没有登录,请[登录]或[注册]
* 以上用户言论只代表其个人观点，不代表CSDN网站的观点或立场
个人资料
访问我的空间  
lcyGo
 
访问：525187次
积分：1119
等级： 
排名：千里之外
原创：2篇转载：24篇译文：0篇评论：0条
文章搜索

 搜索
文章分类
BI产品技术框架(1)
JAVA(7)
Machine Learning(2)
中间件(1)
Hadoop(3)
架构(13)
运维(3)
数据(0)
文章存档
2017年09月(32)
2017年08月(2)
2017年07月(8)
阅读排行
Maven创建Web项目(62589)
Java经典设计模式之七大结构型模式（附实例和详解）(59164)
Java垃圾收集学习笔记(57227)
Java内存管理(57216)
Java经典设计模式之十一种行为型模式（附实例和详解）(55245)
Java经典设计模式之五大创建型模式（附实例和详解）(51222)
机器学习 数据特征分析 特征工程(50218)
JVM类加载原理学习笔记(48290)
Java开发中的23种设计模式详解(转)(47322)
机器学习 训练方法选择指导图 备忘单(43982)
评论排行
Spark踩坑记――Spark Streaming+Kafka(0)
机器学习 训练方法选择指导图 备忘单(0)
机器学习 数据特征分析 特征工程(0)
Java垃圾收集学习笔记(0)
JVM类加载原理学习笔记(0)
Java内存管理(0)
Java经典设计模式之十一种行为型模式（附实例和详解）(0)
Java经典设计模式之七大结构型模式（附实例和详解）(0)
Java经典设计模式之五大创建型模式（附实例和详解）(0)
Java开发中的23种设计模式详解(转)(0)
推荐文章
* CSDN新版博客feed流内测用户征集令
* Android检查更新下载安装
* 动手打造史上最简单的 Recycleview 侧滑菜单
* TCP网络通讯如何解决分包粘包问题
* SDCC 2017之大数据技术实战线上峰会
* 快速集成一个视频直播功能


公司简介|招贤纳士|广告服务|联系方式|版权声明|法律顾问|问题报告|合作伙伴|论坛反馈
网站客服杂志客服微博客服webmaster@csdn.net400-660-0108|北京创新乐知信息技术有限公司 版权所有|江苏知之为计算机有限公司|江苏乐知网络技术有限公司
京 ICP 证 09002463 号|Copyright ? 1999-2017, CSDN.NET, All Rights Reserved GongshangLogo

登录 | 注册
关闭

hero_fantao的专栏
 目录视图 摘要视图 订阅
异步赠书：9月重磅新书升级，本本经典           程序员9月书讯      每周荐书：ES6、虚拟现实、物联网（评论送书）
 余额支付风控--整体篇
2015-03-07 22:08 2173人阅读 评论(0) 收藏 举报
 分类： 风控（8）  
版权声明：本文为博主原创文章，未经博主允许不得转载。
目录(?)[+]
余额支付风控
整体篇
by dylanfan at 2015-2-9
一  概述
什么是风控？在支付行业的的风控是通过一定手段来对平台的业务风险进行控制，偏业务安全领域。风控和技术安全还是不太一样，技术安全更多关注系统漏洞，是否被攻击，是否被拖库。 所以，风控更加注重对业务的理解，数据分析，模型，以及风险打击策略的制定。一句话：支付领域的风控就是来之别当前交易是否有风险，如果有，进行什么的动作。 在支付领域，支付产品出来前，都会和风控团队沟通，提前预知可能的风险点，并保证用户体验的情况下，制定风险策略。
二 余额支付场景
场景大体分为购买行为（包括余额支付，混合支付，合单支付，代付），C2C 转账，付款到银行卡，提现。所有风险控制策略基本都是围绕这几块场景的风险情况来制定的。

各个场景具体情况如下：
（1）购买行为： 买方是用户，卖方是商户，购买的是物品。物品有虚拟，自动发货，一般实物，高危实物等。根据物品属性不同，风险也不同。销赃快速和方便的物品的风险相对较高，比如虚拟和自动发货的物品。常见的虚拟物品：游戏道具，充手机话费等。所以，这块的风险一方面是是从买方账号侧来看，一方面从物品属性来看。
（2）C2C转账：买方是用户，卖方也是用户。风险可以从买卖双方两侧账号情况来看。
（3）付款到银行卡是主要指钱从余额转到一张银行卡（非账号绑定的卡）中，常见的如汇款转账，还信用卡。这块的风险可以买方账号侧和银行卡信息来看。
（4）提现：指余额的钱提现到自己绑定的卡中。相对前面三种情况，对于账号被盗销赃场景的风险，提现相对最低。
三 用户分类

1： 按证书用户分：无证书用户和数字证书用户。无证书用户是余额被盗对抗中最为激烈的一块。
2：按一般用户和C商户账号来分：普通用户和拍拍卖家账号。普通用户骗子常用的手法是批量处理销赃。拍拍卖家账号，金额很多，骗子经常一对一的钓鱼来处理。对拍拍卖家账号的交易处理尤为谨慎。
3：按实名来分：非实名用户和实名用户；实名只需要账号姓名和身份证姓名一致就行。
4：按实名认证来分：非实名认证用户和实名认证用户。实名认证是在实名的基础上需要有银行卡信息，这块的处理有两种方式：绑卡开通快捷和系统给你指定的银行卡打很小的钱，你回答打了多少钱来确定银行卡是你本人的。
四 风险评估和惩罚手段
   余额支付风险主要分两块：（1）盗号的风险 （2）盗卡的风险。盗号的风险：账号被盗，余额被转移的风险。盗卡的风险：骗子骗到银行卡后，注册个账号，用该银行卡开通快捷支付充值到余额，然后通过余额支付的风险。 两者情况不同，识别的方法也不同。

4.1：业务限制
支付反欺诈一直是个长期博弈的过程，此消彼长，不太可能我们能完全控制住对方，取得压倒性胜利，很多反欺诈策略是事后根据损失案例来制定的，所以，基本上一个新的支付产品（购买，转账，红包等都可以看成一种支付产品）出来，在风险异常识别策略之前，都应该有个基本的业务限制策略（这个需要和产品协商，在不影响用户的支付体验上做限制），比如限定一定额度或者实名认证等，这样可以把损失上限控制在一定的范围内。
4.2：风险异常识别
风险异常识别是从损失案例中的分析和对比正常样本来发现异常特征，然后制定风险策略，风险策略包括识别风险和如何打击。风险识别方法主要分为两块专家规则体系和风向模型体系。规则精准度高，覆盖率低，上线快速，但系统迁移和维护成本高。模型覆盖率高，精准度相对单一规则要低，需要足够多的样本，但具备学习能力，系统迁移和维护便利。对于反欺诈领域对规则和模型的效果对比可以参考如下：

   无论专家规则还是模型，首先都是要挖掘足够多的有用的风险特征。风险评估基本可以总结为识别异常和过滤正常，所以风险特征集合基本就包含这两块的特征。在盗号风险异常中，基本面临的就是账号被盗在异地销赃和同城销赃，异地相对同城识别容易些。欺诈和反欺诈的对抗博弈会不断加剧，从简单的对抗向深层次的对抗延伸，坏人盗号后的支付行为也会越来越伪装的很像正常行为。所以，我们也需要挖掘更深次特征来区别异常支付行为和正常行为，否则，就会带来拦截量高涨。我相信，坏人伪装的再好，他和正常用户自己的行为总会有不一样的地方。具体的识别方法和不同用户在不同场景的风控策略这里不细说，后续具体探讨。
4.3：惩罚手段
限额，加验（根据用户的自己的定义来选择是加验数字证书，还是U顿，短信等），实名认证，拦截交易，关闭余额，冻结账号，加黑信息等。不同的行为评估，采用不同惩罚手段。对于惩罚手段需要注意的是，如果涉及到金额，对单笔金额限制意义是不大的，最好用累计金额来限制。同时，一旦识别有风险点，需要在各个场景进行布控，任何场景的布控缺失都是高危的。风控遵循木桶原理，风控的水平取决于你对最薄弱环节的控制。对于控制手段，我想提下黑产。在黑产市场上有个词是叫信封，信封就是装有账号和密码的文件。每个信封都有它价格。同样，黑产上每个银行卡号也有相应的价格，在我们把额度控制在一定范围内，也能起到不错的作用，比如黑产上一张卡的价格是500元，我们控制的额度在500元以下，骗子花大功夫去却收益很少，长期来看，对骗子也能起到驱赶的作用。  
五 策略上线和运营
5.1: 策略上线
分析损失案例，对比正常交易，我们发现恶意手法，制定相应的风险策略。首先这个风险策略，是经过常规离线评估指标，准确率，覆盖率多少。离线评估指标达标后，我们就可以尝试放到线上了。但是在真实环境中真正应用风险策略前，我们需要把策略先放到测试环境上来观察一段时间，评估拦截量和准确度。相对广告一些领域，算法和模型测试都是做A/B Test不同，风控的测试环境和真实环境保持一致，只需在测试环境中给命中策略的交易打上相应的标记来记录就好，不做真实动作，就可以起到测试的作用。
策略上线后，风控策略和欺诈者的博弈对抗就会开始。刚开始的时候，策略的准确度都会很高，但是随着时间推移，策略的准确性就会下降。因为，欺诈者之前的欺诈手法所做的交易行为为拦截，并多次碰壁后，他们就会意识到，他们的行为已经被监测了，之前发现的风控系统的漏洞已经被堵上，他们会随之消停直到发现新的风控系统漏洞再发起攻击。
5.2：策略运营
风险策略运营需要观察拦截曲线和损失曲线。时间周期可以以周为基本单位，观察一定时间内拦截曲线的变化和损失曲线的变化，详细记录拦截曲线大变化的事件或策略，尤其是是损失曲线上涨和下降原因。
策略上线后，我们需要对策略拦截量，准确率等指标有个实时监控。通过监控，我们能发现拦截异常，策略当前有效性问题等。对策略有效性监控可以有如下几个方法：抽样调查，用户反馈，白名单机制（看白名单被你命中的比率）。在盗号领域，用户反馈可以分为：要求加验，看用户是否加验成功；或者冻结账户，用户来反馈当前交易是否是本人操作等。损失曲线变化一般相对策略上线时间有个几天的延时，主要是由于用户反馈有个延时期。
                         
顶
2
 
踩
0
 
 
上一篇Kaggle ： Display Advertising Challenge( ctr 预估 )
下一篇余额支付风控 -- 风控评分模型篇
  相关文章推荐
? 风控建模（高级）分析师
? 自然语言处理在“天猫精灵”的实践应用--姜飞俊
? 支付系统风控系统建设思考
? 蚂蜂窝大数据平台架构及Druid引擎实践--汪木铃
? 四步教你：开发风控模型？
? Retrofit 从入门封装到源码解析
? 量化还是风控？
? 程序员如何转型AI工程师
? 笔记蚍缈胤掷嗄Ｐ椭掷啵决策、排序）比较与模型评估体系（ROC/gini/KS/lift）
? 深入探究Linux/VxWorks的设备树
? 互联网风控模型需要多大数据？
? 使用QEMU搭建u-boot+Linux+NFS嵌入式开发环境
? 风控评分模型
? 《Credit Risk Scorecard》 第六章: Scorecard Development
? 余额支付风控 -- 风控评分模型篇
? Android属性动画实现TextView类似支付宝余额数字滚动

查看评论

  暂无评论

您还没有登录,请[登录]或[注册]
* 以上用户言论只代表其个人观点，不代表CSDN网站的观点或立场
个人资料
访问我的空间  
范涛
 
访问：87989次
积分：1411
等级： 
排名：千里之外
原创：59篇转载：0篇译文：2篇评论：25条
文章搜索

 搜索
文章分类
编程(16)
机器学习(33)
推荐系统(9)
风控(9)
深度学习(7)
Spark(1)
Kaggle(4)
复杂网络 | 大规模网络挖掘(6)
文章存档
2017年09月(1)
2017年04月(12)
2017年03月(6)
2016年04月(1)
2015年03月(2)
展开
阅读排行
Kaggle ： Display Advertising Challenge( ctr 预估 )(16438)
LIME：模型预测结果是否值得信任？(5177)
连续特征离散化和归一化(4825)
OWL-QN算法： 求解L1正则优化(3319)
余额支付风控 -- 风控评分模型篇(2974)
二分类问题特征选择的常用两个方法(2428)
微博好友推荐算法-SALSA(2201)
余额支付风控--整体篇(2170)
不平衡学习方法理论和实战总结(2105)
Loan default predictor（贷款违约预测）(1970)
评论排行
Kaggle ： Display Advertising Challenge( ctr 预估 )(9)
利用BeatifulSoup包学习爬虫，抓取《今晚看啥》电影评分和标签(4)
FaceBook: Text Tag Recommendation(3)
OWL-QN算法： 求解L1正则优化(3)
《Credit Risk Scorecard》第五章： Development Database Creation(2)
LIME：模型预测结果是否值得信任？(2)
海量数据相似查找系列2 -- Annoy算法(1)
连续特征离散化和归一化(1)
Google Smart Reply笔记： Automated Response Suggestion for Email(1)
读论文： 行为在高聚类网络中传播更快(0)
推荐文章
* CSDN新版博客feed流内测用户征集令
* Android检查更新下载安装
* 动手打造史上最简单的 Recycleview 侧滑菜单
* TCP网络通讯如何解决分包粘包问题
* SDCC 2017之大数据技术实战线上峰会
* 快速集成一个视频直播功能
最新评论
《Credit Risk Scorecard》第五章： Development Database Creation
范涛: @qq_33454405:看你需求，如果你只关注最终相对顺序的话，其实最终结果可以不用调整。但是如果...
Google Smart Reply笔记： Automated Response Suggestion for Email
范涛: 稍后找时间，用tensorflow或者keras 把其中其中封闭集合的生成模型实现和验证下效果
《Credit Risk Scorecard》第五章： Development Database Creation
qq_33454405: 对于预测概率调整有以下疑问：因为评分卡模型大多数是样本不均衡的，比如1：13，如果我在lr模型里设置...
海量数据相似查找系列2 -- Annoy算法
winnie6713: 简直是雪中送炭，最近也在看这个算法然后运用它对图片进行搜索。
LIME：模型预测结果是否值得信任？
ziwunimelb: 感谢分享
连续特征离散化和归一化
大号小白兔: 请问一下，输入数据的格式是什么样的呢？
LIME：模型预测结果是否值得信任？
吴士龙: 感谢分享
FaceBook: Text Tag Recommendation
carfieldfei: 你好，我对这个题目非常感兴趣，用了你的方法，cv的结果只有不到0.46。有几个细节的问题请教一下：1...
Kaggle ： Display Advertising Challenge( ctr 预估 )
zhangzhengyuan123123: 楼主，你的xgboost部分 max_depth':10, 这个本身不建议超过8的，你选择10，是在...
Kaggle ： Display Advertising Challenge( ctr 预估 )
soappp: 楼主，facebook那篇文章我也看了，但始终没理解，原有样本经过GBDT后，怎么输入到LR的。每个...


公司简介|招贤纳士|广告服务|联系方式|版权声明|法律顾问|问题报告|合作伙伴|论坛反馈
网站客服杂志客服微博客服webmaster@csdn.net400-660-0108|北京创新乐知信息技术有限公司 版权所有|江苏知之为计算机有限公司|江苏乐知网络技术有限公司
京 ICP 证 09002463 号|Copyright ? 1999-2017, CSDN.NET, All Rights Reserved GongshangLogo

















登录 | 注册
关闭

爱吃鱼油
 目录视图 摘要视图 订阅
异步赠书：9月重磅新书升级，本本经典           程序员9月书讯      每周荐书：ES6、虚拟现实、物联网（评论送书）
 支付风控模型
标签： 风控
2017-06-29 16:32 122人阅读 评论(0) 收藏 举报
 分类： 金融（5）  
目录(?)[+]
支付风控涉及到多方面的内容，包括反洗钱、反欺诈、客户风险等级分类管理等。 其中最核心的功能在于对实时交易进行风险评估，或者说是欺诈检测。如果这个交易的风险太高，则会执行拦截。由于反欺诈检测是在交易时实时进行的，在要求不能误拦截的同时，还有用户体验上的要求，即不能占用太多时间，一般要求风控操作必须控制在100ms以内，对于交易量大的业务，10ms甚至更低的性能要求都是必须的。 这就需要对风控模型进行合理的设计。一般来说，要提升风控的拦截效率，就需要考虑更多的维度，但这也会带来计算性能的下降。在效率和性能之间需要进行平衡。

本文重在介绍建立风控模型的方法，每个公司应该根据自己的实际业务情况和开发能力来选择合适的模型。这里列出来的模型仅为了说明问题，提供参考。

一、风险等级

做风控拦截，首先要回答的问题是风险等级怎么划分？ 目前主流的风险等级划分有三种方式， 三等级、四等级、五等级。

三等级的风险分为 低风险、中风险和高风险。 大部分交易是低风险的，不需要拦截直接放行。 中风险的交易是需要进行增强验证，确认是本人操作后放行。 高风险的交易则直接拦截。

四风险等级，会增加一个中高风险等级。此类交易在用户完成增强验证后，还需要管理人员人工核实，核实没问题后，交易才能放行。

五风险等级，会增加一个中低风险等级。此类交易是先放行，但是管理人员需要进行事后核实。 如果核实有问题，通过人工方式执行退款，或者提升该用户的风险等级。



大部分支付系统是使用三等级的风险。

二、基于规则的风控

规则是最常用的，也是相对来说比较容易上手的风控模型。从现实情况中总结出一些经验，结合名单数据，制定风控规则，简单，有效。 常见的规则有：

1. 名单规则

使用白名单或者黑名单来设置规则。具体名单如上文所述，包括用户ID、IP地址、设备ID、地区、公检法协查等。 比如：

用户ID是在风控黑名单中。

用户身份证号在反洗钱黑名单中。

用户身份证号在公检法协查名单中。

用户所使用的手机号在羊毛号名单列表中。

转账用户所在地区是联合国反洗钱风险警示地区。

2. 操作规则

对支付、提现、充值的频率按照用户账号、IP、设备等进行限制，一旦超出阈值，则提升风控等级。

频率需综合考虑（五）分钟、（一）小时、（一）天、（一）周等维度的数据。由于一般计算频率是按照自然时间段来进行的，所以如果用户的操作是跨时间段的，则会出现频率限制失效的情况。 当然，比较复杂的可以用滑窗来做。

对不同的风险等级设置不同的阈值。 比如：

用户提现频次5分钟不能超过2次， 一小时不能超过5次，一天不能超过10次。

用户提现额度一天不能超过1万。

用户支付频次5分钟不能超过2次，一小时不能超过10次，一天不能超过100次。

3. 业务规则

和特定各业务相关的一些规则，比如：

同一个人绑定银行卡张数超过10张。

同一张银行卡被超过5个人绑定。

同一个手机号被5个人绑定。

一个周内手机号变更超过4次。

同一个对私银行卡接受转账次数一分钟超过5次。

4. 行为异常

用户行为和以前的表现不一致，比如：

用户支付地点与常用登录地点不一致

用户支付使用个IP与常用IP地址不一致

用户在短时间内，上一次支付的地址和本次支付的地址距离非常远。 比如2分钟前在中国支付的，2分钟后跑到美国去支付了。

5. 风控拦截历史规则

用户在某个业务上的消费行为被风控网关多次拦截。

规则引擎优点：

性能高： 对订单按照规则进行匹配，输出结果。一般不会涉及到复杂的计算。

易于理解和分析： 交易被拦截到底是触犯了那条规则，很容易输出。

开发相对简单。

规则引擎存在的问题：

一刀切，容易被薅羊毛的人嗅探到。比如规则规定超过5000元就进行拦截，那羊毛号会把订单拆分成4999元来做。 一天限制10笔，那就薅到9笔就停手了。

规则冲突问题。当一笔交易命中IP白名单和额度黑名单的时候应该如何处理？

规则引擎看起来简单，但也是最实用的一类模型。 它是其它风控模型的基础。实践中，首先使用已知的规则来发现存在问题的交易，人工识别交易的风险等级后，把这些交易作为其它有监督学习的训练数据集。

三、决策树模型

风险评估从本质上来说是一个数据分类问题。 和传统的金融行业风险评估不一样的地方，在于数据规模大、业务变化快、实时要求高。一旦有漏洞被发现，会对公司造成巨大损失。 而机器学习是解决这些问题的利器。 互联网金融风控离不开机器学习，特别是支付风控。 在各种支付风控模型中，决策树模式是相对比较简单易用的模型。 如下的决策树模型，我们根据已有的数据，分析数据特征，构建出一颗决策树。当有一笔交易发生时，我们使用决策树来判断这笔交易是否是高风险交易。这种模型的优点是非常容易理解，检测速度快。 因而也是现有机构中常用的模型之一。 风控模型存在的主要问题是其产生的结果比较粗略。同样的两个交易被判定为高风险，究竟哪种交易风险更高，决策树模型无法给出答案。

四、评分模型

比决策树模型更进一步，现在也有不少公司在使用评分（卡）模型。 银行在处理信用风险评级、反洗钱风险等级时，往往也是使用这种方法。

每个公司的模型都不一样，一个参考模型如下：

该模型为参考《金融机构洗钱和恐怖融资风险评估及客户分类管理指引》编制，仅具参考意义。虽然银行间的评分模型有很好的参考价值，但互联网公司由于业务和数据的不同，评分模型参考价值不大。

每个公司需根据自己的业务情况来制定评分模型，之后为各个指标指定权重比例。 权重评分结果为0~100分的区间，之后按照区间划分，指定风险等级。比如：

当然，评分区间也需要根据企业的实际情况来制定。 评分模型的优势在于：

性能比较高，针对交易进行指标计算，按照区间来确定风险。

相对于规则，如果指标设置合理，其覆盖度高， 不容易被嗅探到漏洞。

理解和分析也比较容易。 如果交易被拦截了，可以根据其各项打分评估其被拦截的原因。

存在的问题：

模型真的很难建立。指标的选择是一个挑战。

各个参数的调优是一个长期的过程。

我们知道从一条交易记录中可以挖掘的关联数据有上百个，衍生数据就更多了。比如从支付地址，可以聚类出常用地址，衍生出当前地址和常用地址、上一次支付地址之间的距离，而这些指标在构建模型时都可能使用到。 所以第一个问题是，如何从这些指标中建立一个合适的模型？这就涉及到机器学习的问题了。 模型不能凭空建立，我们可以通过规则来对现有数据进行筛选和标注，确定这些记录集的风险等级。 这些数据作为样本来训练模型。可用的算法包括Apriori、FP-growth等。算法实现请参考相关文档。

在确认相关参数后，模型在使用过程中还需要不断对相关参数进行调整。这是一个拟合或者回归的算法，Logistic算法、CART算法，可以用来对参数做调优。

总之，模型的建立是一个不断学习、优化的过程。 而每一个模型的发布，还需要进行试运行，AB测试和上线。 这个过程，将在下一篇的风控架构中介绍。

五、模型评估

风控本质上是对交易记录的一个分类，所以对风控模型的评估，除了性能外，还需要评估“查全率”和“查准率”。 如下图所示：



以评估高风险人群的效果为例，

Precision, 准确率，也叫查准率，指模型发现的真实的高风险人数占模型发现的所有高风险人数的比例。

Recall，召回率，也叫查全率，指模型发现的真实的高风险人数占全部真实的高风险人数的比例。

理想情况下，我们希望这两个指标都要高。实际上，往往是互斥的，准确率高、召回率就低，召回率低、准确率高。如果两者都低，那就是模型不靠谱了。 对于风控来说，需要在保证准确率的情况下，尽量提高召回率。 那怎么发现实际的高风险人数呢？ 这就需要借助规则模型，先过滤一遍，再从中人工遴选。

从实际应用情况来看，目前国内大部分团队使用Logistic回归+评分模型来做风控，少数人使用决策树。国外的PayPal是支付平台风控的标杆，国内前海征信、蚂蚁金服等会使用到更高级的神经网络和机器学习，但实际效果未见到实证材料。

支付风控场景分析 ；

支付风控数据仓库建设 ；

支付风控模型和流程分析(本文)；

支付风控系统架构

转载：http://www.tuicool.com/articles/2qyUvmM

顶
0
 
踩
0
 
 
上一篇eclipse下载
下一篇Xshell 5 如何方便巧妙的打开FTP？
  相关文章推荐
? 支付风控模型
? 自然语言处理在“天猫精灵”的实践应用--姜飞俊
? 4步教你开发风控评分模型
? 蚂蜂窝大数据平台架构及Druid引擎实践--汪木铃
? 金融风控-->申请评分卡模型-->申请评分卡介绍
? Retrofit 从入门封装到源码解析
? 金融风控-->客户流失预警模型-->特征工程
? 程序员如何转型AI工程师
? 金融风控-->客户流失预警模型-->神经网络建模
? 深入探究Linux/VxWorks的设备树
? 【大数据部落】基于随机森林、svm、CNN机器学习的风控欺诈识别模型
? 使用QEMU搭建u-boot+Linux+NFS嵌入式开发环境
? 金融风控-->客户流失预警模型-->金融数据分析
? 金融风控-->客户流失预警模型-->GBDT建模
? 金融风控-->申请评分卡模型-->特征工程（特征分箱，WOE编码）
? 金融风控-->申请评分卡模型-->logisticRegression建模

查看评论

  暂无评论

您还没有登录,请[登录]或[注册]
* 以上用户言论只代表其个人观点，不代表CSDN网站的观点或立场
个人资料
访问我的空间  
爱吃鱼油
 
 3
访问：147115次
积分：3507
等级： 
排名：第9934名
原创：188篇转载：154篇译文：0篇评论：8条
文章搜索

 搜索
文章分类
Spring(30)
Hibernate(4)
mybatis(10)
JDBC(5)
Struts2.5(1)
java(57)
服务器(8)
数据库(28)
JQuery(34)
maven(14)
JNDI(2)
axis(4)
SQL(8)
Junit(4)
xml(1)
设计模式(7)
Dubbo(4)
Linux(16)
Velocity(2)
httpclient(2)
Transaction事务(8)
quartz(4)
web调试(4)
AOP(1)
html5(5)
json(11)
java加密解密(4)
webSQL(3)
extjs(1)
AngularJS(3)
js(26)
ETL(1)
浏览器缓存(1)
存储过程(Procedure)(3)
eclipse(9)
消息中间件MQ(4)
webservice(5)
计算机网路(2)
开发工具(11)
数据结构复习题(2)
Socket(1)
other(1)
jpa(5)
CSS(3)
Spring Boot(2)
nginx(8)
代码检查(4)
jsp(1)
权限(1)
工作流(0)
JTA(1)
redis(4)
多线程(12)
元注解(1)
泛型(1)
java反射(1)
java虚拟机(7)
H5(2)
网络(1)
集合(2)
安全(5)
mysql(8)
金融(6)
zookeeper(2)
gradle(0)
superdiamond(1)
版本控制(6)
定时任务(5)
日志(3)
构建+持续集成(5)
文章存档
2017年09月(9)
2017年08月(18)
2017年07月(20)
2017年06月(42)
2017年05月(3)
展开
阅读排行
Oracle操作ORA-02289: 序列不存在 解决方案(8403)
js前端3des加密 后台java解密(7074)
循环冗余校验码计算CRC(3222)
checking for C compiler ... not found(3074)
JPA的查询语言―使用原生SQL(2772)
当spring 容器初始化完成后执行某个方法(2251)
We trust you have received the usual lecture from the local System Administrator. It usually boils(2194)
spring @Entity @Table(1792)
filter过滤器过滤特殊字符(1754)
Spring整合metaq(1728)
评论排行
juery笔记常用代码(3)
js前端3des加密 后台java解密(2)
jquery类选择器及其遍历(1)
git版本恢复命令reset(1)
HashMap(1)
Junit4单元测试(1)(0)
maven中pom.xml中添加对axis2的支持(0)
tomcat下的JNDI配置(0)
Jenkins远程部署 gitLab配置(0)
tomcat启动startup.bat一闪而过(0)
推荐文章
* CSDN新版博客feed流内测用户征集令
* Android检查更新下载安装
* 动手打造史上最简单的 Recycleview 侧滑菜单
* TCP网络通讯如何解决分包粘包问题
* SDCC 2017之大数据技术实战线上峰会
* 快速集成一个视频直播功能
最新评论
git版本恢复命令reset
shengjmm: 写得很不错。。
HashMap
Phoebe星丫头: hashmap学习了
js前端3des加密 后台java解密
xiazhifu123: 非常感谢大神
jquery类选择器及其遍历
吴士龙: 感谢分享
js前端3des加密 后台java解密
石头匠人: 感谢
juery笔记常用代码
快乐柠檬: 常记笔记是个很好的习惯，加油！
juery笔记常用代码
爱吃鱼油: 谢谢
juery笔记常用代码
盟盟哒: 不错，加油


公司简介|招贤纳士|广告服务|联系方式|版权声明|法律顾问|问题报告|合作伙伴|论坛反馈
网站客服杂志客服微博客服webmaster@csdn.net400-660-0108|北京创新乐知信息技术有限公司 版权所有|江苏知之为计算机有限公司|江苏乐知网络技术有限公司
京 ICP 证 09002463 号|Copyright ? 1999-2017, CSDN.NET, All Rights Reserved GongshangLogo



登录 | 注册
关闭

尹]水的博客
只是为了掌握更多东西
目录视图摘要视图订阅
异步赠书：9月重磅新书升级，本本经典           程序员9月书讯      每周荐书：ES6、虚拟现实、物联网（评论送书）
 支付风控：今天下午做了一个风控的东西，分享出来！
2017-08-14 18:49 140人阅读 评论(0) 收藏 举报
 分类： SpringMVC（1）   项目
版权声明：你随便转，没坑你就行！
其中使用了springBoot，里面还做了一些工具类，大部分是比较大小

package com.banma.wechat.service;

import java.math.BigDecimal;
import java.sql.Timestamp;
import java.util.*;

import javax.annotation.Resource;

import com.banma.wechat.model.*;
import net.sf.json.JSONObject;
import org.springframework.beans.factory.annotation.Value;

/**
* 交易和提现风控措施
* @author Administrator
*
*/
public class RiskMeasureService {

@Resource
private MerchantInfoService merchantInfoService;

@Resource
private CollectiveTransOrderService collectiveTransOrderService;

@Resource
private RiskService riskService;

@Resource
private RiskRecordService riskRecordService;

@Value("${risk.payRule1}")
private String payRule1;

@Value("${risk.payRule2}")
private String payRule2;

@Value("${risk.payRule3}")
private String payRule3;

@Value("${risk.payRule4}")
private String payRule4;

@Value("${risk.payRule5}")
private String payRule5;


@Value("${risk.withdrawRule1}")
private String withdrawRule1;

@Value("${risk.withdrawRule2}")
private String withdrawRule2;

@Value("${risk.withdrawRule3}")
private String withdrawRule3;

/**
* 交易风险控制
* 商户编号
* 订单
* @return
* @throws Exception
*/
public Map<String, Object> tradeRisk(CollectiveTransOrder collectiveTransOrder)throws Exception{

Map<String, Object> msg = new HashMap<String,Object>();
List<RiskRule> rules = riskService.queryAllRisk();//所有的规则
MerchantInfo merchantInfo = merchantInfoService.findMerchantInfoByMerchantNo(collectiveTransOrder.getMerchantNo());//商户个人信息

Map<Integer,RiskRule> s = new HashMap<Integer,RiskRule>();
for(RiskRule r :rules){
if(r.getStatus().equals(1)){//表示激活
s.put(r.getRuleNo(),r);
}
}

//第一条：连续24小时内出现多笔相同交易金额
if(s.containsKey(payRule1)){//激活
Map<String,Object> rule11 = payTimesRisk(collectiveTransOrder,merchantInfo,s.get(payRule1));
if(rule11.get("success") instanceof Boolean==false){
msg.put("success", false);
msg.put("msg", rule11.get("msg"));
return msg;
}
}

//第二条：新注册的商户刷卡金额不得超过100，上传图片资源通过才可以
if(s.containsKey(payRule2)){//已经开启
Map<String,Object> rule22 = newRegistPayLimitRisk(collectiveTransOrder,merchantInfo,s.get(payRule2));
if(rule22.get("success") instanceof Boolean==false){
msg.put("success", false);
msg.put("msg", rule22.get("msg"));
return msg;
}
}

//第三条： 交易时间的限制1，比如：1:00到12：00

if(s.containsKey(payRule3)){//
Map<String,Object> rule33 = payLimitTime(collectiveTransOrder,merchantInfo,s.get(payRule3));
if(rule33.get("success") instanceof Boolean==false){//交易时间不通过
msg.put("success", false);
msg.put("msg", "时间1："+rule33.get("time"));
return msg;
}
}

//第四条： 交易时间的限制2，比如：6:00到8：00

if(s.containsKey(payRule4)){//
Map<String,Object> rule44 = payLimitTime(collectiveTransOrder,merchantInfo,s.get(payRule4));
if(rule44.get("success") instanceof Boolean==false){//交易时间不通过
msg.put("success", false);
msg.put("msg", "时间2："+rule44.get("time"));
return msg;
}
}

//第五条： 交易时间的限制3，比如：14:00到16：00

if(s.containsKey(payRule5)){//
Map<String,Object> rule44 = payLimitTime(collectiveTransOrder,merchantInfo,s.get(payRule5));
if(rule44.get("success") instanceof Boolean==false){//交易时间不通过
msg.put("success", false);
msg.put("msg", "时间3："+rule44.get("time"));
return msg;
}
}

msg.put("success",true);
return msg;

}


/**
* 提现风险控制
* @return
* @throws Exception
*/
Map<String, Object> widthdrawRisk(SettleOrderInfo settleOrderInfo) throws Exception{

Map<String, Object> msg = new HashMap<String,Object>();
List<RiskRule> rules = riskService.queryAllRisk();//所有的规则
MerchantInfo merchantInfo = merchantInfoService.findMerchantInfoByMerchantNo(settleOrderInfo.getSettleUserNo());//商户个人信息
Map<Integer,RiskRule> s = new HashMap<Integer, RiskRule>();
for(RiskRule r :rules){
if(r.getStatus().equals(1)){//表示激活
s.put(r.getRuleNo(),r);
}
}

//第一条：单笔金额是多少
if(s.containsKey(withdrawRule1)){

Map<String,Object> withRule1 = PayLimitByAmount(withdrawRule1,merchantInfo,settleOrderInfo);
if(withRule1.get("success") instanceof Boolean==false){//交易时间不通过
msg.put("success", false);
msg.put("msg",withRule1.get("msg"));
return msg;
}

}

//第二条：审核通过
if(s.containsKey(withdrawRule2)){
Map<String,Object> withRule2 = MerchantShenhePass(merchantInfo,withdrawRule2);
if(withRule2.get("success") instanceof Boolean==false){//交易时间不通过
msg.put("success", false);
msg.put("msg", withRule2.get("msg"));
return msg;
}
}

return msg;
}



/**
* 支付次数控制：当前时候之前的24小时之内，不允许发起支付3笔数字相同的金额订单
* 处理措施：冻结商户一段时间
*/
public Map<String,Object> payTimesRisk(CollectiveTransOrder collectiveTransOrder,MerchantInfo merchantInfo,RiskRule riskRule){

//这里要对这个字段进行处理：全部使用json格式,ruleValue只存储json格式的信息
// 格式如下：{"Differ":"5"}表示额度摆动是 5元！
JSONObject json = JSONObject.fromObject( riskRule.getRulesValues());

Map<String,Object> msg = new HashMap<String,Object>();
//1.查询一天内的该商户的订单
Timestamp currentTime = new Timestamp(new Date().getTime());
Timestamp currentTimeBefore = new Timestamp(new Date().getTime()-3600*24*1000);
List<CollectiveTransOrder> orders = collectiveTransOrderService.findDayTransOrderByMerchantNo(merchantInfo.getMerchantNo(),currentTime,currentTimeBefore);
//对比客户24小时之前，刷卡金额：
List<String> amountList = new ArrayList<String>();

for(CollectiveTransOrder c:orders){
if(Integer.valueOf(c.getAccount())>Integer.valueOf(json.get("limitAmount").toString())){//将额度大于这个值得额度集合起来
amountList.add(c.getAccount());
}
}
//支付金额：额度差
int size = approximationByList(collectiveTransOrder.getAccount(),amountList,riskRule);

if(size>Integer.valueOf(json.get("SameSize").toString())){

merchantInfo.setStatus("2");
merchantInfoService.update(merchantInfo);

RiskRecord record = new RiskRecord();
record.setRiskNo(riskRule.getRuleNo().toString());
record.setMerchantNo(merchantInfo.getMerchantNo());
record.setMerchantName(merchantInfo.getMerchantName());
record.setCreateTime(new Timestamp(new Date().getTime()));
record.setRiskDescription("交易金额存在异常：24小时内存在"+size+"笔交易，当前限制交易次数是"+json.get("SameSize"));

riskRecordService.insertRecord(record);

msg.put("success",false);
msg.put("msg","交易金额存在异常：24小时内存在"+size+"笔交易，当前限制交易次数是"+json.get("SameSize"));
return msg;
}else{
msg.put("success",true);
}

return msg;
}


/**
* 新注册的商户刷卡金额不得超过100，上传图片资源通过才可以
* @param merchantInfo
* @param rule
* @return
*/
public Map<String,Object> newRegistPayLimitRisk(CollectiveTransOrder collectiveTransOrder,MerchantInfo merchantInfo,RiskRule rule){

//1.查询商户的图片认证状态
JSONObject json = JSONObject.fromObject( rule.getRulesValues());
Map<String,Object> msg = new HashMap<String,Object>();

if(!merchantInfo.getStatus().equals(1)){//商户状态不正常

if(Integer.valueOf(collectiveTransOrder.getAccount())>Integer.valueOf(json.get("limitAmount").toString())){//商户未通过，大于刷卡额度

RiskRecord record = new RiskRecord();
record.setRiskNo(rule.getRuleNo().toString());
record.setMerchantNo(merchantInfo.getMerchantNo());
record.setMerchantName(merchantInfo.getMerchantName());
record.setCreateTime(new Timestamp(new Date().getTime()));
record.setRiskDescription("商户未通过：刷卡金额大于"+json.get("limitAmount"));

riskRecordService.insertRecord(record);

msg.put("success",false);
msg.put("msg","商户未通过：刷卡金额大于"+json.get("limitAmount"));
return msg;
}else{
msg.put("success",true);
}
}else{
msg.put("success",true);
}
return msg;
}

/**
* 交易时间限制
* @param rule
* @return
*/
private Map<String,Object> payLimitTime(CollectiveTransOrder collectiveTransOrder,MerchantInfo merchantInfo,RiskRule rule) {

JSONObject json = JSONObject.fromObject( rule.getRulesValues());
Map<String,Object> msg = new HashMap<String,Object>();

Timestamp currentTime = new Timestamp(new Date().getTime());

//1.时间不能在这个时间段:大于最大值，小于最小值
String start = json.get("startTime").toString();
String end = json.get("endTime").toString();
boolean flag = CheckTimeFromStartToEnd(currentTime,start,end);

if(!flag){

RiskRecord record = new RiskRecord();
record.setRiskNo(rule.getRuleNo().toString());
record.setMerchantNo(merchantInfo.getMerchantNo());
record.setMerchantName(merchantInfo.getMerchantName());
record.setCreateTime(new Timestamp(new Date().getTime()));
record.setRiskDescription("当前时间："+currentTime+"不可交易");

riskRecordService.insertRecord(record);

msg.put("success",false);
msg.put("msg","当前时间："+currentTime+"不可交易");
}
return msg;
}

/**
* 提现金额限制
* @param withdrawRule1
* @return
*/
private Map<String,Object> PayLimitByAmount(String withdrawRule1,MerchantInfo merchantInfo,SettleOrderInfo settleOrderInfo) {

Map<String,Object> msg = new HashMap<String,Object>();
RiskRule rule = riskService.queryRiskByRuleNo(withdrawRule1);

BigDecimal payMoney = settleOrderInfo.getSettleAmount();
BigDecimal limitMoney = new BigDecimal(rule.getRulesValues());
if(payMoney.compareTo(limitMoney)==1){

RiskRecord record = new RiskRecord();
record.setRiskNo(rule.getRuleNo().toString());
record.setMerchantNo(merchantInfo.getMerchantNo());
record.setMerchantName(merchantInfo.getMerchantName());
record.setCreateTime(new Timestamp(new Date().getTime()));
record.setRiskDescription("提现金额："+payMoney+"，超过"+limitMoney);

msg.put("success",false);
msg.put("msg","提现金额："+payMoney+"，超过"+limitMoney);
}else{
msg.put("success",true);
}

return msg;
}


/**
* 商户审核
* @param merchantInfo
* @param withdrawRule2
* @return
*/
private Map<String,Object> MerchantShenhePass(MerchantInfo merchantInfo, String withdrawRule2) {

Map<String,Object> map = new HashMap<String,Object>();

RiskRule rule = riskService.queryRiskByRuleNo(withdrawRule2);
if(merchantInfo.getStatus()!="1"){

RiskRecord record = new RiskRecord();
record.setRiskNo(rule.getRuleNo().toString());
record.setMerchantNo(merchantInfo.getMerchantNo());
record.setMerchantName(merchantInfo.getMerchantName());
record.setCreateTime(new Timestamp(new Date().getTime()));
record.setRiskDescription("商户状态未通过");

map.put("success",false);
map.put("msg","商户状态未通过");
}else{

map.put("success",true);
}
return map;
}

/**
* 通过一个值，找到在list中的相似值数量
* @param account
* @param amountList
* @return
*/
private int approximationByList(String account, List<String> amountList,RiskRule riskRule) {

String differ = riskRule.getRulesEngine();//近似值

List<String> list = new ArrayList<String>();

for(String s: amountList){
BigDecimal acountSub = new BigDecimal(account).subtract(new BigDecimal(differ));
BigDecimal accouAdd = new BigDecimal(account).add(new BigDecimal(differ));
BigDecimal Amount = new BigDecimal(s);

if(Amount.compareTo(acountSub)>1&&Amount.compareTo(accouAdd)<1){
list.add(s);
}

}

return list.size();
}

/**
* 判断当前时间是否在此段中
* @param currentTime
* @param start
* @param end
* @return
*/
private boolean CheckTimeFromStartToEnd(Timestamp currentTime, String start, String end) {
//2017-08-14 17:29:30.983
String begin = start.substring(start.indexOf(":")-2,start.lastIndexOf(":")+3);
String close = end.substring(end.indexOf(":")-2,end.lastIndexOf(":")+3);
String current= currentTime.toString().substring(currentTime.toString().indexOf(":")-2,currentTime.toString().lastIndexOf(":")+3);

Long beginTime = (long)Integer.valueOf(begin.substring(0,2))*3600+Integer.valueOf(begin.substring(3,5))+Integer.valueOf(begin.substring(6,8));
Long closeTime = (long)Integer.valueOf(close.substring(0,2))*3600+Integer.valueOf(close.substring(3,5))+Integer.valueOf(close.substring(6,8));
Long currtTime = (long)Integer.valueOf(current.substring(0,2))*3600+Integer.valueOf(current.substring(3,5))+Integer.valueOf(current.substring(6,8));

if(currtTime>=beginTime&&currtTime<=closeTime){//在区间中
return false;
}
return true;
}



}
顶
0
踩
0
 
 
上一篇2017/08/14 springBoot和SpringCloud学习之路（一）
  相关文章推荐
? 今天下午要进行一个经验分享――公司季度大会（半年会）
? 自然语言处理在“天猫精灵”的实践应用--姜飞俊
? 今天做了一个 web 应用发布共大家参考
? 蚂蜂窝大数据平台架构及Druid引擎实践--汪木铃
? 那天在CSDN上看到一个网友在华为遇到的面试题是一道乘法题看似简其实并不是因为它们都超出了数据类型的范围，今天做了下加法希望对大家有所启示
? Retrofit 从入门封装到源码解析
? 今天看到猪八戒网的一个动态的文本框，心血来潮自己也做了一个
? 程序员如何转型AI工程师
? 简单的伪静态，看了很多 自己做了一个简单的jsp的伪静态，和大家分享哈，其实很简单的
? 深入探究Linux/VxWorks的设备树
? 今天用MongoDB给用户做了个小插件，借鉴于实战开发 【零基础学习，附完整Asp.net示例】 写的非常不错
? 使用QEMU搭建u-boot+Linux+NFS嵌入式开发环境
? 做了20年股票白活了，今天终于体验了一次合法“抢银行”！
? 今天做了freemaker 导出word文档 的bug修复,解决 \n换行 问题
? 今天做了个java解析EXCLE文件作为邮件内容发送
? 今天的Coursera测验做了三次才对这说明对Precision和Recall的理解有问题

查看评论

  暂无评论

您还没有登录,请[登录]或[注册]
* 以上用户言论只代表其个人观点，不代表CSDN网站的观点或立场
个人资料
 访问我的空间 
电竞女神丨陈二姐
 
访问：21349次
积分：284
等级： 
排名：千里之外
原创：7篇转载：5篇译文：3篇评论：4条
文章搜索

搜索
文章分类
J2SE基础(4)
TCP/IP(0)
JavaScript(0)
JQuery(0)
AJAX(0)
JSON(0)
SQL基础教程(0)
XML(0)
Linux(0)
Servlet(0)
Jsp(0)
正则表达式(0)
MySQL(0)
Oracle(0)
Redis(0)
Eclipse/Myeclipse(1)
Git/Maven(0)
Spring(2)
SpringMVC(2)
Shiro(0)
JPA(0)
Hibernate(1)
MyBatis/ibatis(0)
项目(1)
开篇寄语(0)
开发工具(4)
开发工具类(2)
操作系统(0)
windows(0)
idea(0)
sts(0)
springBoot(0)
springCloud(1)
文章存档
2017年08月(2)
2017年06月(1)
2016年11月(2)
2016年10月(10)
阅读排行
eclipse版本对应的jdk需求(15349)
Myeclipse导出项目制作成一个可执行的jar(2233)
关于hibernate自动建表(758)
这是网上搜到的，认为还可以的对集合的总结，更多请往原网址查看(536)
Myeclipse的debug的使用(443)
spring4.1.6使用定时器quartz(250)
定时任务(245)
SpringMVC(224)
字符串相关方法(216)
2017/08/14 springBoot和SpringCloud学习之路（一）(149)
评论排行
eclipse版本对应的jdk需求(4)
web.xml配置文件中<async-supported>true</async-supported>报错的解决方案(0)
SpringMVC(0)
排序工具类(0)
字符串相关方法(0)
文件上传辅助类(0)
项目中的一些工具类(0)
定时任务(0)
Myeclipse的debug的使用(0)
这是网上搜到的，认为还可以的对集合的总结，更多请往原网址查看(0)
推荐文章
* CSDN新版博客feed流内测用户征集令
* Android检查更新下载安装
* 动手打造史上最简单的 Recycleview 侧滑菜单
* TCP网络通讯如何解决分包粘包问题
* SDCC 2017之大数据技术实战线上峰会
* 快速集成一个视频直播功能
最新评论
eclipse版本对应的jdk需求
stvrandolph: 不错的
eclipse版本对应的jdk需求
gsls200808: 4.4可以用JDK6，虽然官网说要用JDK7，博主可以测试下
eclipse版本对应的jdk需求
brucefu4: 不错哦，谢谢哈


公司简介|招贤纳士|广告服务|联系方式|版权声明|法律顾问|问题报告|合作伙伴|论坛反馈
网站客服杂志客服微博客服webmaster@csdn.net400-660-0108|北京创新乐知信息技术有限公司 版权所有|江苏知之为计算机有限公司|江苏乐知网络技术有限公司
京 ICP 证 09002463 号|Copyright ? 1999-2017, CSDN.NET, All Rights Reserved GongshangLogo

CSDN首页
学院
下载
更多
下载 CSDN APP
写博客
登录|注册
csdn首页移动开发架构云计算/大数据互联网运维数据库前端编程语言研发管理综合全部 
原来支付宝、财付通每天都是这样对账、风控的！
转载 2015年10月05日 18:31:52 223601
为了可以更好地解释支付结算系统对账过程，我们先把业务从头到尾串起来描述一下场景，帮助大家理解：一个可能得不能再可能的场景，请大家深刻理解里面每个角色做了什么，获取了哪些信息：
　　某日阳光灿烂，支付宝用户小明在淘宝上看中了暖脚器一只，价格100元。犹豫再三后小明使用支付宝网银完成了支付，支付宝显示支付成功，淘宝卖家通知他已发货，最近几日注意查收。
　　小明：持卡人，消费者，淘宝和支付宝的注册会员，完成了支付动作，自己的银行账户资金减少，交易成功。
　　银行：收单银行，接受来自支付宝的名为“支付宝BBB”的100元订单，并引导持卡人小明支付成功，扣除小明银行卡账户余额后返回给支付宝成功通知，并告诉支付宝这笔交易在银行流水号为“银行CCC”
　　支付宝：支付公司，接收到淘宝发来的订单号为“淘宝AAA”的商户订单号，并形成支付系统唯一流水号：“支付宝BBB”发往银行系统。然后获得银行回复的支付成功信息，顺带银行流水号“银行CCC”
　　淘宝：我们支付公司称淘宝这类电商为商户，是支付系统的客户。淘宝向支付系统发送了一笔交易收单请求，金额为100，订单号为:“淘宝AAA”，支付系统最后反馈给淘宝这笔支付流水号为“支付BBB”
　　以上流程貌似大家都达到了预期效果，但实际上仍然还有一个问题：
　　对支付公司（支付宝）而言，虽然银行通知了它支付成功，但资金实际还要T+1后结算到它银行账户，所以目前只是一个信息流，资金流还没过来。
　　Tips:插一句话，对支付系统内部账务来说，由于资金没有能够实时到账，所以此时小明的这笔100元交易资金并没有直接记入到系统资产类科目下的“银行存款”科目中，而是挂在“应收账款”或者“待清算科目”中。大白话就是，这100元虽然答应给我了，我也记下来了，但还没收到，我挂在那里。
　　对商户（淘宝）而言，虽然支付公司通知了它支付成功，他也发货了，但资金按照合同也是T+1到账。如果不对账确认一下，恐怕也会不安。
　　倒是对消费者（小明）而言：反正钱付了，你们也显示成功了，等暖脚器呀等暖脚器~
　　基于支付公司和商户的困惑，我们的支付结算系统需要进行两件事情：一是资金渠道对账，通称对银行帐；二是商户对账，俗称对客户帐。对客户帐又分为对公客户和对私客户，通常对公客户会对对账文件格式、对账周期、系统对接方案有特殊需求，而对私客户也就是我们一般的消费者只需要可以后台查询交易记录和支付历史流水就可以了。
　　我们先聊银行资金渠道对账，由于支付公司的资金真正落地在商业银行，所以资金渠道的对账显得尤为重要。
　　在一个银行会计日结束后，银行系统会先进行自己内部扎帐，完成无误后进行数据的清分和资金的结算，将支付公司当日应入账的资金结算到支付公司账户中。于此同时，目前多数银行已经支持直接系统对接的方式发送对账文件。于是，在某日临晨4点，支付宝系统收到来自银行发来的前一会计日对账文件。根据数据格式解析正确后和前日支付宝的所有交易数据进行匹配，理想情况是一一匹配成功无误，然后将这些交易的对账状态勾对为“已对账”。
　　Tips:此时，对账完成的交易，会将该笔资金从“应收账款”或者“待清算账款”科目中移动到“银行存款”科目中，以示该交易真正资金到账。
　　以上太理想了，都那么理想就不要对账了。所以通常都会出一些差错，那么我总结了一下常见的差错情况：
　　1.支付时提交到银行后没有反馈，但对账时该交易状态为支付成功
　　这种情况很正常，因为我们在信息传输过程中，难免会出现掉包和信息不通畅。消费者在银行端完成了支付行为，银行的通知信息却被堵塞了，如此支付公司也不知道结果，商户也不知道结果。如果信息一直没法通知到支付公司这边，那么这条支付结果就只能在日终对账文件中体现了。这时支付公司系统需要对这笔交易进行补单操作，将交易置为成功并完成记账规则，有必要还要通知到商户。
　　此时的小明：估计急的跳起来了……付了钱怎么不给说支付成功呢！坑爹！
　　TIPS：通常银行系统会开放一个支付结果查询接口。支付公司会对提交到银行但没有回复结果的交易进行间隔查询，以确保支付结果信息的实时传达。所以以上情况出现的概率已经很少了。
　　2.我方支付系统记录银行反馈支付成功，金额为100，银行对账金额不为100
　　这种情况已经不太常见了，差错不管是长款和短款都不是我们想要的结果。通常双方系统通讯都是可作为纠纷凭证的，如果银行在支付结果返回时确认是100元，对账时金额不一致，可以要求银行进行协调处理。而这笔账在支付系统中通常也会做对应的挂账处理，直到纠纷解决。
　　3.我方支付系统记录银行反馈支付成功，但对账文件中压根不存在
　　这种情况也经常见到，因为跨交易会计日的系统时间不同，所以会出现我们认为交易是23点59分完成的，银行认为是第二天凌晨0点1分完成。对于这种情况我们通常会继续挂账，直到再下一日的对账文件送达后进行对账匹配。
　　如果这笔交易一直没有找到，那么就和第二种情况一样，是一种短款，需要和银行追究。
　　以上情况针对的是一家银行资金渠道所作的流程，实际情况中，支付公司会在不同银行开立不同银行账户，用以收单结算（成本会降低），所以真实情况极有可能是：
　　临晨1点，工行对账文件丢过来（支行A）
　　临晨1点01分，工行又丢一个文件过来（支行B）
　　临晨1点15分，农行对账文件丢过来
　　。 。 。
　　临晨5点，兴业银行文件丢过来
　　。。。
　　不管什么时候，中国银行都必须通过我方业务员下载对账文件再上传的方式进行对账，所以系统接收到中行文件一般都是早上9点05分……
　　对系统来说，每天都要处理大量并发的对账数据，如果在交易高峰时段进行，会引起客户交互的延迟和交易的失败，这是万万行不得的所以通常支付公司不会用那么傻的方式处理数据，而是在一个会计日结束后，通常也是临晨时段，将前一日交易增量备份到专用对账服务器中，在物理隔绝环境下进行统一的对账行为，杜绝硬件资源的抢占。
　　以上是银行资金渠道入款的对账，出款基本原理一致，不过出款渠道在实际业务过程中还有一些特别之处，由于大家不是要建设系统，我就不赘述了。
　　谈完了资金渠道的对账，我们再来看看对客户帐。
　　前面提到了，由于资金落在银行，所以对支付公司来说，对银行帐非常重要。那么同理，由于资金落在支付公司，所以对商户来说，对支付公司账也一样重要。能否提供高品质甚至定制化的服务，是目前支付公司走差异化路线的一个主要竞争点。
　　之前说过，银行与支付公司之间的通讯都是可以作为纠纷凭证的。原理是对支付报文的关键信息进行密钥加签+md5处理，以确保往来报文“不可篡改，不可抵赖”。
　　同理，支付公司和商户之间也会有类似机制保证报文的可追溯性，由此我们可以知道，一旦我方支付系统通知商户支付结果，那么我们就要为此承担责任。由此我们再推断一个结论：
　　即便某支付订单银行方面出错导致资金未能到账，一旦我们支付系统通知商户该笔交易成功，那么根据协议该结算的资金还是要结算给这个商户。自然，我们回去追究银行的问题，把账款追回。
　　没经过排版的小知识点---------------------------------------------------
　　一、对支付系统而言，最基本的对账功能是供商户在其后台查询下载某一时间段内的支付数据文件，以供商户自己进行对账。
　　这个功能应该是个支付公司就有，如果没有就别混了。
　　二、对大多数支付系统而言，目前已经可以做到对账文件的主动投送功能。
　　这个功能方便了商户系统和支付系统的对接，商户的结算人员无须登录支付平台后台下载文件进行对账，省去了人工操作的麻烦和风险。
　　对大型支付系统而言，商户如果跨时间区域很大，反复查询该区域内的数据并下载，会对服务器造成比较大的压力。各位看官别笑，觉得查个数据怎么就有压力了。实际上为了这个查询，我最早就职的一家支付公司重新优化了所有SQL语句，并且因为查询压力过大服务器瘫痪过好几次。
　　现在比较主流的做法是把商户短期内查询过、或者经常要查询的数据做缓存。实在不行就干脆实时备份，两分钟同步一次数据到专用数据库供商户查询，以避免硬件资源占用。甚至……大多数支付公司都会对查询范围跨度和历史事件进行限制，比如最多只能查一个月跨度内，不超过24个月前的数据……以避免服务嗝屁。
　　对账这块大致就这样了，再往细的说就说不完了，
　　风险控制，在各行各业都尤其重要。
　　金融即风险，控制好风险，才有利润。
　　虽然第三方支付严格意义上说并非属于金融行业，但由于涉及资金的清分和结算，业务主体又是资金的收付，所以风险控制一样非常重要。
　　对支付公司来说，风控主要分为合规政策风控以及交易风控两种。
　　前者主要针对特定业务开展，特定产品形态进行法规层面的风险规避，通常由公司法务和风控部门一起合作进行。例如，一家公司要开展第三方支付业务，现在要获得由人民银行颁发的“支付业务许可证”。遵守中国对于金融管制的所有条规，帮助人行监控洗钱行为……这些法规合规风险，虽然条条框框，甚至显得文绉绉，但如果没人解读没人公关，业务都会无法开展。
　　当然，这块也不是本题所关注的重点，提问者关注的，应当是业务进行过程中的交易风控环节。
　　现在随着各支付公司风险控制意识的加强，风控系统渐渐被重视起来。除了上述提到的合规风控相关功能，风控系统最讲技术含量，最讲业务水平，最考究数据分析的业务就是交易风控环节。
　　对一笔支付交易而言，在它发生之前、发生过程中及发生过程后，都会被风控系统严密监控，以确保支付及客户资产安全。而所有的所有秘密，都归结到一个词头上：规则。风控系统是一系列规则的集合，任何再智能的风控方案，都绕不开规则二字。
　　我们先看看，哪些情况是交易风控需要监控处理的：
　　1.钓鱼网站
　　什么是钓鱼呢？
　　用我的说法，就是利用技术手段蒙蔽消费者，当消费者想付款给A的时候，替换掉A的支付页面，将钱付给B，以达成非法占用资金的目的。
　　还有更低级的，直接就是发小广告，里面带一个类似http://tiaobao.com的网址，打开后和淘宝页面一摸一样，上当客户直接付款给假冒网站主。
　　第一种情况风控系统是可以通过规则进行简单判定的，虽然有误杀，但不会多。
　　通常使用的规则是判断提交订单的IP地址和银行实际支付完成的IP地址是否一致，如果不一致，则判断为钓鱼网站风险交易，进入待确认订单。
　　但第二种情况，亲爹亲娘了，支付公司真的没办法。当然遇到客户投诉后可能会事后补救，但交易是无法阻止了。
　　2.盗卡组织利用盗卡进行交易
　　大家都知道，信用卡信息是不能随便公布给别人的，国内大多信用卡虽然都设置了密码，但银行仍然会开放无磁无密支付接口给到商户进行快速支付之用。
　　所谓无磁无密，就是不需要磁道信息，不需要密码就可以进行支付的通道。只需要获取到客户的CVV，有效期，卡号三个核心信息，而这三个信息是在卡上直接有的，所以大家不要随便把卡交给别人哦~
　　碰到类似的这种交易，风控系统就不会像钓鱼网站这样简单判断了。
　　过去我们所有的历史交易，都会存库，不仅会存支付相关信息，更会利用网页上的控件（对，恶心的activex或者目前用的比较多的flash控件）抓取支付者的硬件信息，存储在数据库中。
　　当一笔交易信息带着能够搜集到的硬件信息一同提交给风控系统时，风控系统会进行多种规则判定。
　　例如：当天该卡是否交易超过3次
　　当天该IP是否交易超过3次
　　该主机CPU的序列号是否在黑名单之列
　　等等等等，一批规则跑完后，风控系统会给该交易进行加权评分，标示其风险系数，然后根据评分给出处理结果。
　　通过硬件信息采集以及历史交易记录回溯，我们可以建立很多交易风控规则来进行监控。所以规则样式的好坏，规则系数的调整，就是非常重要的用以区别风控系统档次高低的标准。
　　例如，我听说著名的风控厂商RED，有一个“神经网络”机制，灰常牛逼。
　　
　　其中有一个规则我到现在还记忆犹新：
　　某人早上八点在加利福尼亚进行了信用卡支付，到了下午一点，他在东亚某小国家发起了信用卡支付申请。系统判断两者距离过长，不是短短5小时内能够到达的，故判定交易无效，支付请求拒绝。
　　规则非常重要，当然，数据也一样重要。我们不仅需要从自己的历史记录中整合数据，同时还要联合卡组织、银行、风控机构，购买他们的数据和风控服务用来增加自己的风控实力。
　　SO，风控是一个不断积累数据、分析数据、运营数据、积累数据的循环过程。
　　好的风控规则和参数，需要经过无数次的规则修改和调整，是一个漫长的过程。
　　不知道大家做互联网，有没有利用GA做过AB测试，同样的，风控系统也需要反复地做类似AB测试的实验，以保证理论和实际的匹配。
　　最后给大家说一个小小的概念：
　　所谓风控，是指风险控制，不是风险杜绝。
　　风控的目标一定不是把所有风险全部杜绝。
　　合理的风控，目标一定是：利润最大化，而不是风险最小化
　　过于严格的风控规则，反而会伤害公司利益（看看销售和风控经常打架就知道了）
　　不光是交易的风控，我们日常制定规则，法规，公司流程，也一定要秉着这个出发点进行规划。
阅读全文
举报
目前您尚未登录，请 登录 或 注册 后进行评论
相关文章推荐
看大型的支付系统如支付宝等如何实现风控的！

为了可以更好地解释支付结算系统对账过程，我们先把业务从头到尾串起来描述一下场景，帮助大家理解： 一个可能得不能再可能的场景，请大家深刻理解里面每个角色做为了可以更好地解释支付结算系统对账过程，我们先把...
muyutingfeng2008muyutingfeng20082015-06-23 17:102035
SODBASE CEP学习进阶篇（七）续：SODBASE CEP与Spark streaming集成-低延迟规则管理

许多大数据平台项目采用流式计算来处理实时数据，会涉及到一个环节：实时数据处理规则管理。因为用户经常有自己配置数据处理规则或策略的需求。同时，维护人员来也有也有将规则提取出来的需求，方便变更和维护的需求...
happynyearhappynyear2015-12-01 11:24698
 
《程序员看天下》实战：揭秘携程大数据的应用处理

一直以来，携程拥有海量数据，如何存储、分析和应用这些数据一直是部门痛点所在！携程大数据团队将会给出什么样的解决方案呢？开源产品的选型和运维又该如何抉择呢....
SODBASE CEP学习进阶篇（七）：SODBASE CEP与Spark streaming集成

基于内存RDD的Spark框架在很多新项目中被使用。Spark计算框架包括其Streaming组件，出发点是批处理层(Lamda架构中Batch Layer)划分批的思路。在许多地方，用户在使用Spa...
happynyearhappynyear2015-11-07 17:19914
支付宝，财付通，到底每天都是怎样工作的？

为了可以更好地解释支付结算系统对账过程，我们先把业务从头到尾串起来描述一下场景，帮助大家理解：一个可能得不能再可能的场景，请大家深刻理解里面每个角色做了什么，获取了哪些信息：　　某日阳光灿烂，支付宝用...
scribblerscribbler2015-07-15 07:481165
分享2016支付宝，财付通，微信免签约即时到账接口

支付宝财付通即时到帐源码，不需要mysql数据库。 上传到了空间直接使用，不需要mysql数据库。 上传好了，请修改pay.php里面的收款账号 适合单页，网络赚钱，虚拟...
zouming16888zouming168882016-11-05 22:553303
开发如何选择。。。网银在线，快钱，支付宝，财付通

转载百度文库。 做B2C半年了，用了4个支付网关。写点东西做个比较。   云网因为程序不正规，直接排除掉了。还有贝宝因为亲身见过有人因为漏洞被骗钱，也直接过滤掉了。   现在用网银在线，快钱，支付宝，...
wangyonglin1123wangyonglin11232014-06-26 16:571239
WordPress收费下载资源插件 vip会员功能/收费下载/收费查看/联盟推广+前端用户中心 支付宝/财付通/贝宝/网银/微信[更新至v9.0.2]

Foxpay是一款资源商城插件， 可以给你的用户分享一些收费资源，wordpress插件（Foxpay），经过完美测试运行于wordpress 3.0.1-4.5版本。本插件特点：高级VIP会员系统...
wpzhutiwpzhuti2016-05-17 21:001087

免签约财付通支付宝即时到账辅助工具免签约收款工具

2014-10-09 16:141021KB
下载
WordPress中文插件 Erphpdown vip会员+推广提成+收费下载/查看内容+前端个人中心 银联/支付宝/微信支付/财付通/贝宝paypal[更新至v9.6.1]

会员推广下载专业版 WordPress插件是一款针对虚拟资源收费下载的插件，基于收费下载资源插件，经过完美测试运行于wordpress 3.1.x-4.x版本。后续会增加更多实用的功能。 模板兔已针对...
huaivehuaive2016-07-07 15:031252

易支付 - 财付通 支付宝免签约即时到账自动充值网站V1.34

2016-06-22 11:294.82MB
下载

银联/支付宝/微信支付/财付通 erphpdown

2017-01-14 18:141.54MB
下载

c#版支付宝、财付通、快钱支付

2014-11-18 08:131.81MB
下载

net中主流在线支付接口文档和示例(包括：Paypal,支付宝，财付通)

2010-12-21 21:553.12MB
下载

支付宝，财付通，快钱支付接口.rar

2011-11-22 14:099.36MB
下载

WordPress插件银联/支付宝/微信支付/财付通/贝宝paypal[更新至v9.0.1]

2017-01-14 18:211.54MB
下载

支付宝支付微信支付财付通支付免签约即时到帐辅助接口

2017-08-07 19:00838KB
下载

一元云购程序3套模板 云支付 支付宝 财付通 易宝 微信支付

2016-09-18 15:0611.88MB
下载

财付通支付宝获取转账信息

2013-07-08 17:41296KB
下载

免签约支付宝、财付通插件

2015-07-15 12:071.21MB
下载

2017wordpress支付宝插件+集成支付宝担保交易+及时到账&amp;财付通+银联(原版v3.4)

2017-04-23 22:06104KB
下载
 
muyutingfeng2008

＋关注
原创
8
 
粉丝
0
 
喜欢
0
 
码云
未开通
他的最新文章更多文章
银联统一规范的收单业务消息域
技术揭秘二：探讨12306两地三中心混合云架构
技术揭秘一：12306是如何实现高流量高并发的关键技术？
第三方支付平台漏洞多 消费者1个月内被19次盗刷

编辑推荐
最热专栏
看大型的支付系统如支付宝等如何实现风控的！
SODBASE CEP学习进阶篇（七）续：SODBASE CEP与Spark streaming集成-低延迟规则管理
SODBASE CEP学习进阶篇（七）：SODBASE CEP与Spark streaming集成
支付宝，财付通，到底每天都是怎样工作的？
分享2016支付宝，财付通，微信免签约即时到账接口
在线课程

自然语言处理在“天猫精灵”的实践应用
自然语言处理在“天猫精灵”的实践应用
讲师：姜飞俊
蚂蜂窝大数据平台架构及Druid引擎实践
蚂蜂窝大数据平台架构及Druid引擎实践
讲师：汪木铃


喜欢
 
收藏
 
评论
 
分享
 

登录 | 注册
李博Garvin的专栏
阿里云机器学习PD
 目录视图 摘要视图 订阅
异步赠书：9月重磅新书升级，本本经典           程序员9月书讯      每周荐书：ES6、虚拟现实、物联网（评论送书）
 [置顶] 【机器学习PAI实践四】如何实现金融风控
标签： 机器学习算法数据
2017-04-18 17:08 1697人阅读 评论(0) 收藏 举报
 分类： 机器学习（25）  
版权声明：本文为博主原创文章，未经博主允许不得转载。
目录(?)[+]
（本文数据为虚构，仅供实验）

一、背景

本文将针对阿里云平台上图算法模块来进行实验。图算法一般被用来解决关系网状的业务场景。与常规的结构化数据不同，图算法需要把数据整理成首尾相连的关系图谱。图算法更多的是考虑边和点的概念。阿里云机器学习平台上提供了丰富的图算法组件，包括K-Core、最大联通子图、标签传播聚类等。 
本文的业务场景如下： 
下图是已知的一份人物通联关系图，每两个人之间的连线表示两人有一定关系，可以是同事关系或者亲人关系等。已知“Enoch”是信用用户，”Evan”是欺诈用户，计算出其它人的信用指数。通过图算法，可以算出图中每个人是欺诈用户的概率，这个数据可以方便相关机构做风控。



二、数据集介绍

数据源：本文数据为自己生成，用于实验。 
具体字段如下：

字段名	含义	类型	描述
start_point	边的起始节点	string	人
end_point	边结束节点	string	人
count	关系紧密度	double	数值越大，两人的关系越紧密
数据截图： 


三、数据探索流程

首先，实验流程图： 


1.最大联通子图

最大联通子图的功能很好理解，前面已经介绍了，图算法的输入数据是关系图谱结构的。最大联通子图可以找到有通联关系的最大集合，在团伙发现的场景中可以排除掉一些与风控场景无关的人。本次实验通过“最大联通子图”组件将数据中的群体分为两部分，并赋予group_id。通过“SQL脚本”组件和“JOIN”组件去除下图中的无关联人员。 


2.单源最短路径

通过“单源最短路径”组件探查出每个人的一度人脉、二度人脉关系等。distance讲的是“Enoch”通过几个人可以联络到目标人。 
如下图： 


3.标签传播分类

“标签传播分类”算法为半监督的分类算法，原理是用已标记节点的标签信息去预测未标记节点的标签信息。在算法执行过程中，每个节点的标签按相似度传播给相邻节点。 
调用“标签传播分类”组件除了要有所有人员的通联图数据以外，还要有人员打标数据。这里通过“已知数据-读odps”组件导入打标数据(weight表示目标是欺诈用户的概率)： 


通过SQL对结果进行筛选，最终结果展现的是每个人涉嫌欺诈的概率，数值越大表示是欺诈用户的概率越大。 


四、其它

参与讨论：云栖社区公众号

免费体验：阿里云数加机器学习平台

顶
0
 
踩
0
 
 
上一篇【机器学习PAI实践三】雾霾成因分析
下一篇【机器学习PAI实践五】机器学习眼中的《人民的名义》
  相关文章推荐
? 机器学习在金融大数据风险建模中的应用
? 自然语言处理在“天猫精灵”的实践应用--姜飞俊
? 【机器学习PAI实践四】如何实现金融风控
? 蚂蜂窝大数据平台架构及Druid引擎实践--汪木铃
? 【机器学习PAI实践六】金融贷款发放预测
? Retrofit 从入门封装到源码解析
? 【机器学习PAI实践十】深度学习Caffe框架实现图像分类的模型训练
? 程序员如何转型AI工程师
? 【机器学习PAI实践二】人口普查统计
? 深入探究Linux/VxWorks的设备树
? 【机器学习PAI实践十二】机器学习算法基于信用卡消费记录做信用评分
? 使用QEMU搭建u-boot+Linux+NFS嵌入式开发环境
? 7月28日云栖精选夜读：【上报纸啦】95后大学生用机器学习PAI大战老年痴呆
? 【机器学习PAI实践十一】机器学习PAI为你自动写歌词，妈妈再也不用担心我的freestyle了（提供数据、代码
? 机器学习算法与Python实践之（四）支持向量机（SVM）实现
? 机器学习算法与Python实践之（四）支持向量机（SVM）实现
查看评论

  暂无评论

您还没有登录,请[登录]或[注册]
* 以上用户言论只代表其个人观点，不代表CSDN网站的观点或立场
我的微信公众号
作者公众号：凡人机器学习
凡人机器学习
作者新书《机器学习实践应用》
主要讲述算法和业务的结合，适合初学者
机器学习实践应用
京东地址
个人资料
访问我的空间  
李博Garvin
 
 5  4
访问：700477次
积分：9760
等级： 
排名：第1884名
原创：219篇转载：39篇译文：0篇评论：436条
勿忘初心―coding for dream，for fun，for freedom 
Name:Garvin Li
My Website:
www.garvinli.com
My Github:
https://github.com/jimenbian
Email:
garvinli@garvinli.com
Abstract: 
曾经从事移动开发（android相关），现在主要从事机器学习算法的实现，如果有兴趣的同学，不妨联系我，参加我的X-Brain开源项目。
我的开源项目：
ospaf(开源项目成熟度分析工具)
X-Brain(机器学习算法)
Android-ScreenShot(android系统截屏)
媒体采访: 
CSDN的报道 
云栖社区的报道
友情链接
czxttkl的专栏
wusuopu的专栏
buptpatriot的专栏
文章搜索

 搜索
博客专栏
	机器学习实践
文章：12篇
阅读：17441
	LeetCode从零单排
文章：31篇
阅读：34579
	git学习笔记
文章：5篇
阅读：7190
	机器学习算法-python实现
文章：14篇
阅读：114145
	android-tips
文章：20篇
阅读：88775
	Cocos2d实例教程
文章：8篇
阅读：21480
文章分类
linux(11)
c语言(2)
java(49)
c#(12)
百度地图api(5)
学习笔记(62)
web互联网(3)
android开发(25)
DataMining(28)
Cocos2d实例教程(8)
J2EE-ssh(2)
算法与数据结构(47)
JDBC(3)
开源夏令营(13)
python(16)
git(5)
面试(5)
SQL(3)
Hadoop(1)
分布式计算(3)
shell(1)
机器学习(25)
阅读排行
【机器学习算法-python实现】逻辑回归的实现(LogicalRegression)(17459)
【机器学习算法-python实现】扫黄神器-朴素贝叶斯分类器的实现(15907)
【机器学习算法-python实现】决策树-Decision tree（1） 信息熵划分数据集(15566)
Android系统截屏的实现（附代码）(15112)
android tesseract-ocr实例教程（包含中文识别）（附源码）(14820)
【机器学习算法-python实现】KNN-k近邻算法的实现（附源码）(12043)
【机器学习算法-python实现】决策树-Decision tree（2） 决策树的实现(11476)
【android4.3】记一次完整的android源码截屏事件的捕获（不同于网上的老版本）(10528)
c#中WebBrowser控件的使用方法(10062)
阿里巴巴校招内推简历筛选方案（总结篇）(9149)
推荐文章
* CSDN新版博客feed流内测用户征集令
* Android检查更新下载安装
* 动手打造史上最简单的 Recycleview 侧滑菜单
* TCP网络通讯如何解决分包粘包问题
* SDCC 2017之大数据技术实战线上峰会
* 快速集成一个视频直播功能
文章存档
2017年09月(9)
2017年08月(3)
2017年07月(6)
2017年06月(6)
2017年05月(3)
展开
评论排行
android tesseract-ocr实例教程（包含中文识别）（附源码）(50)
Android系统截屏的实现（附代码）(43)
新闻个性化推荐系统(python)-（附源码 数据集）(20)
【android4.3】记一次完整的android源码截屏事件的捕获（不同于网上的老版本）(18)
android4.3 截屏功能的尝试与失败分析(16)
【码农本色】用数据解读我的2014(14)
阿里巴巴机器学习系列课程(12)
android告别篇-对于源码我的一些看法(11)
明天是我的生日，写给24岁的自己(11)
论这场云盘大战，以及各网盘的优劣(9)
统计
公司简介|招贤纳士|广告服务|联系方式|版权声明|法律顾问|问题报告|合作伙伴|论坛反馈
网站客服杂志客服微博客服webmaster@csdn.net400-660-0108|北京创新乐知信息技术有限公司 版权所有|江苏知之为计算机有限公司|江苏乐知网络技术有限公司
京 ICP 证 09002463 号|Copyright ? 1999-2017, CSDN.NET, All Rights Reserved GongshangLogo

登录 | 注册
关闭

村头陶员外的博客
 目录视图 摘要视图 订阅
异步赠书：9月重磅新书升级，本本经典           程序员9月书讯      每周荐书：ES6、虚拟现实、物联网（评论送书）
 金融风控-->申请评分卡模型-->特征工程（特征分箱，WOE编码）
标签： 金融特征分箱-WOE编码
2017-07-16 21:26 992人阅读 评论(1) 收藏 举报
 分类： 金融风控（6）  
版权声明：本文为博主原创文章，未经博主允许不得转载。
这篇博文主要讲在申请评分卡模型中常用的一些特征工程方法，申请评分卡模型最多的还是logsitic模型。

先看数据，我们现在有三张表：

已加工成型的信息：

Master表 
idx:每一笔贷款的unique key,可以与另外2个文件里的idx相匹配。 
UserInfo_*:借款人特征字段 
WeblogInfo_*:Info网络行为字段 
Education_Info*:学历学籍字段 
ThirdParty_Info_PeriodN_*:第三方数据时间段N字段 
SocialNetwork_*:社交网络字段 
ListingInfo:借款成交时间 
Target:违约标签(1 = 贷款违约,0 = 正常还款)

需要衍生的信息

借款人的登陆信息表 
ListingInfo:借款成交时间 
LogInfo1:操作代码 
LogInfo2:操作类别 
LogInfo3:登陆时间 
idx:每一笔贷款的unique key

这里写图片描述

客户在不同的时间段内有着不同的操作，故我们最好做个时间切片，在每个时间切片内统计一些特征。从而衍生出一些特征。

时间切片:

两个时刻间的跨度

例: 申请日期之前30天内的登录次数
申请日期之前第30天至第59天内的登录次数
基于时间切片的衍生

申请日期之前180天内,平均每月(30天)的登录次数
常用的时间切片

(1、2个)月,(1、2个)季度,半年,1年,1年半,2年
时间切片的选择

不能太长:保证大多数样本都能覆盖到
不能太短:丢失信息
我们希望最大时间切片不能太长，都是最好又能包含大部分信息。那么最大切片应该多大呢？

#coding:utf-8
import pandas as pd
import datetime
import collections
import numpy as np
import random

import matplotlib.pyplot as plt

def TimeWindowSelection(df, daysCol, time_windows):
    '''
    :param df: the dataset containg variabel of days
    :param daysCol: the column of days
    :param time_windows: the list of time window，可分别取30,60,90,,,360
    :return:
    '''
    freq_tw = {}
    for tw in time_windows:
        freq = sum(df[daysCol].apply(lambda x: int(x<=tw))) ##统计在tw时间切片内客户操作的总次数
        freq_tw[tw] = freq/float(len(df))　##tw时间切片内客户总操作数占总的操作数比例
    return freq_tw


data1 = pd.read_csv('PPD_LogInfo_3_1_Training_Set.csv', header = 0)
### Extract the applying date of each applicant
data1['logInfo'] = data1['LogInfo3'].map(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d'))
data1['Listinginfo'] = data1['Listinginfo1'].map(lambda x: datetime.datetime.strptime(x,'%Y-%m-%d'))
data1['ListingGap'] = data1[['logInfo','Listinginfo']].apply(lambda x: (x[1]-x[0]).days,axis = 1)
timeWindows = TimeWindowSelection(data1, 'ListingGap', range(30,361,30))
fig=plt.figure()
ax=fig.add_subplot(1,1,1)
ax.plot(list(timeWindows.keys()),list(timeWindows.values()),marker='o')
ax.set_xticks([0,30,60,90,120,150,180,210,240,270,300,330,360])
ax.grid()
plt.show()
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
这里写图片描述

由上图可以看出，在0-180天的时间切片内的操作数占总的操作数的95%，180天以后的覆盖度增长很慢。所以我们选择180天为最大的时间切片。凡是不超过180天的时间切片，都可以用来做个特征衍生。

选取[7,30,60,90,120,150,180]做为不同的切片,衍生变量。

那么我们来选择提取哪些有用的特征：

统计下LogInfo1和LogInfo2在每个时间切片内被操作的次数m1。
统计下LogInfo1和LogInfo2在每个时间切片内不同的操作次数m2。
统计下LogInfo1和LogInfo2在每个时间切片内m1/m2的值。
time_window = [7, 30, 60, 90, 120, 150, 180]
var_list = ['LogInfo1','LogInfo2']
data1GroupbyIdx = pd.DataFrame({'Idx':data1['Idx'].drop_duplicates()})
for tw in time_window:
    data1['TruncatedLogInfo'] = data1['Listinginfo'].map(lambda x: x + datetime.timedelta(-tw))
    temp = data1.loc[data1['logInfo'] >= data1['TruncatedLogInfo']]
    for var in var_list:
        #count the frequences of LogInfo1 and LogInfo2
        count_stats = temp.groupby(['Idx'])[var].count().to_dict()
        data1GroupbyIdx[str(var)+'_'+str(tw)+'_count'] = data1GroupbyIdx['Idx'].map(lambda x: count_stats.get(x,0))
        # count the distinct value of LogInfo1 and LogInfo2
        Idx_UserupdateInfo1 = temp[['Idx', var]].drop_duplicates()
        uniq_stats = Idx_UserupdateInfo1.groupby(['Idx'])[var].count().to_dict()
        data1GroupbyIdx[str(var) + '_' + str(tw) + '_unique'] = data1GroupbyIdx['Idx'].map(lambda x: uniq_stats.get(x,0))
        # calculate the average count of each value in LogInfo1 and LogInfo2
        data1GroupbyIdx[str(var) + '_' + str(tw) + '_avg_count'] = data1GroupbyIdx[[str(var)+'_'+str(tw)+'_count',str(var) + '_' + str(tw) + '_unique']].\
            apply(lambda x: x[0]*1.0/x[1], axis=1)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
数据清洗

对于类别型变量

        删除缺失率超过50%的变量
        剩余变量中的缺失做为一种状态
对于连续型变量

        删除缺失率超过30%的变量
        利用随机抽样法对剩余变量中的缺失进行补缺
注:连续变量中的缺失也可以当成一种状态

特征分箱（连续变量离散化或类别型变量使其更少类别） 
分箱的定义

将连续变量离散化
将多状态的离散变量合并成少状态
分箱的重要性及其优势

离散特征的增加和减少都很容易，易于模型的快速迭代；
稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；
离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；
特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；
特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。
可以将缺失作为独立的一类带入模型。
将所有变量变换到相似的尺度上。
特征分箱的方法 
　这里写图片描述

这里我们主要讲有监督的卡方分箱法(ChiMerge)。

　　自底向上的(即基于合并的)数据离散化方法。它依赖于卡方检验:具有最小卡方值的相邻区间合并在一起,直到满足确定的停止准则。 
　　基本思想:对于精确的离散化，相对类频率在一个区间内应当完全一致。因此,如果两个相邻的区间具有非常类似的类分布，则这两个区间可以合并；否则，它们应当保持分开。而低卡方值表明它们具有相似的类分布。

分箱步骤： 
这里写图分箱述

这里需要注意初始化时需要对实例进行排序，在排序的基础上进行合并。

卡方阈值的确定：

　　根据显著性水平和自由度得到卡方值 
　　自由度比类别数量小1。例如：有3类,自由度为2，则90%置信度(10%显著性水平)下，卡方的值为4.6。

阈值的意义

　　类别和属性独立时,有90%的可能性,计算得到的卡方值会小于4.6。 
　　大于阈值4.6的卡方值就说明属性和类不是相互独立的，不能合并。如果阈值选的大,区间合并就会进行很多次,离散后的区间数量少、区间大。 
　　 
注: 
1,ChiMerge算法推荐使用0.90、0.95、0.99置信度,最大区间数取10到15之间. 
2,也可以不考虑卡方阈值,此时可以考虑最小区间数或者最大区间数。指定区间数量的上限和下限,最多几个区间,最少几个区间。 
3,对于类别型变量,需要分箱时需要按照某种方式进行排序。

按照最大区间数进行分箱代码：

def Chi2(df, total_col, bad_col, overallRate):
    '''
    :param df: the dataset containing the total count and bad count
    :param total_col: total count of each value in the variable
    :param bad_col: bad count of each value in the variable
    :param overallRate: the overall bad rate of the training set
    :return: the chi-square value
    '''
    df2 = df.copy()
    df2['expected'] = df[total_col].apply(lambda x: x*overallRate)
    combined = zip(df2['expected'], df2[bad_col])
    chi = [(i[0]-i[1])**2/i[0] for i in combined]
    chi2 = sum(chi)
    return chi2


### ChiMerge_MaxInterval: split the continuous variable using Chi-square value by specifying the max number of intervals
def ChiMerge_MaxInterval_Original(df, col, target, max_interval = 5):
    '''
    :param df: the dataframe containing splitted column, and target column with 1-0
    :param col: splitted column
    :param target: target column with 1-0
    :param max_interval: the maximum number of intervals. If the raw column has attributes less than this parameter, the function will not work
    :return: the combined bins
    '''
    colLevels = set(df[col])
    # since we always combined the neighbours of intervals, we need to sort the attributes
    colLevels = sorted(list(colLevels))　## 先对这列数据进行排序，然后在计算分箱
    N_distinct = len(colLevels)
    if N_distinct <= max_interval:  #If the raw column has attributes less than this parameter, the function will not work
        print "The number of original levels for {} is less than or equal to max intervals".format(col)
        return colLevels[:-1]
    else:
        #Step 1: group the dataset by col and work out the total count & bad count in each level of the raw column
        total = df.groupby([col])[target].count()
        total = pd.DataFrame({'total':total})
        bad = df.groupby([col])[target].sum()
        bad = pd.DataFrame({'bad':bad})
        regroup =  total.merge(bad,left_index=True,right_index=True, how='left')##将左侧，右侧的索引用作其连接键。
        regroup.reset_index(level=0, inplace=True)
        N = sum(regroup['total'])
        B = sum(regroup['bad'])
        #the overall bad rate will be used in calculating expected bad count
        overallRate = B*1.0/N　##　统计坏样本率
        # initially, each single attribute forms a single interval
        groupIntervals = [[i] for i in colLevels]## 类似于[[1],[2],[3,4]]其中每个[.]为一箱
        groupNum = len(groupIntervals)
        while(len(groupIntervals)>max_interval):   #the termination condition: the number of intervals is equal to the pre-specified threshold
            # in each step of iteration, we calcualte the chi-square value of each atttribute
            chisqList = []
            for interval in groupIntervals:
                df2 = regroup.loc[regroup[col].isin(interval)]
                chisq = Chi2(df2, 'total','bad',overallRate)
                chisqList.append(chisq)
            #find the interval corresponding to minimum chi-square, and combine with the neighbore with smaller chi-square
            min_position = chisqList.index(min(chisqList))
            if min_position == 0:## 如果最小位置为0,则要与其结合的位置为１
                combinedPosition = 1
            elif min_position == groupNum - 1:
                combinedPosition = min_position -1
            else:## 如果在中间，则选择左右两边卡方值较小的与其结合
                if chisqList[min_position - 1]<=chisqList[min_position + 1]:
                    combinedPosition = min_position - 1
                else:
                    combinedPosition = min_position + 1
            groupIntervals[min_position] = groupIntervals[min_position]+groupIntervals[combinedPosition]
            # after combining two intervals, we need to remove one of them
            groupIntervals.remove(groupIntervals[combinedPosition])
            groupNum = len(groupIntervals)
        groupIntervals = [sorted(i) for i in groupIntervals]　## 对每组的数据安从小到大排序
        cutOffPoints = [i[-1] for i in groupIntervals[:-1]]　## 提取出每组的最大值，也就是分割点
        return cutOffPoints
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
以卡方阈值作为终止分箱条件：

def ChiMerge_MinChisq(df, col, target, confidenceVal = 3.841):
    '''
    :param df: the dataframe containing splitted column, and target column with 1-0
    :param col: splitted column
    :param target: target column with 1-0
    :param confidenceVal: the specified chi-square thresold, by default the degree of freedom is 1 and using confidence level as 0.95
    :return: the splitted bins
    '''
    colLevels = set(df[col])
    total = df.groupby([col])[target].count()
    total = pd.DataFrame({'total':total})
    bad = df.groupby([col])[target].sum()
    bad = pd.DataFrame({'bad':bad})
    regroup =  total.merge(bad,left_index=True,right_index=True, how='left')
    regroup.reset_index(level=0, inplace=True)
    N = sum(regroup['total'])
    B = sum(regroup['bad'])
    overallRate = B*1.0/N
    colLevels =sorted(list(colLevels))
    groupIntervals = [[i] for i in colLevels]
    groupNum  = len(groupIntervals)
    while(1):   #the termination condition: all the attributes form a single interval; or all the chi-square is above the threshould
        if len(groupIntervals) == 1:
            break
        chisqList = []
        for interval in groupIntervals:
            df2 = regroup.loc[regroup[col].isin(interval)]
            chisq = Chi2(df2, 'total','bad',overallRate)
            chisqList.append(chisq)
        min_position = chisqList.index(min(chisqList))
        if min(chisqList) >=confidenceVal:
            break
        if min_position == 0:
            combinedPosition = 1
        elif min_position == groupNum - 1:
            combinedPosition = min_position -1
        else:
            if chisqList[min_position - 1]<=chisqList[min_position + 1]:
                combinedPosition = min_position - 1
            else:
                combinedPosition = min_position + 1
        groupIntervals[min_position] = groupIntervals[min_position]+groupIntervals[combinedPosition]
        groupIntervals.remove(groupIntervals[combinedPosition])
        groupNum = len(groupIntervals)
    return groupIntervals
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
无监督分箱法:

等距划分、等频划分

等距分箱 
　　从最小值到最大值之间,均分为 N 等份, 这样, 如果 A,B 为最小最大值, 则每个区间的长度为 W=(B?A)/N , 则区间边界值为A+W,A+2W,….A+(N?1)W 。这里只考虑边界，每个等份里面的实例数量可能不等。 
　　 
等频分箱 
　　区间的边界值要经过选择,使得每个区间包含大致相等的实例数量。比如说 N=10 ,每个区间应该包含大约10%的实例。 
　　 
以上两种算法的弊端 
　　比如,等宽区间划分,划分为5区间,最高工资为50000,则所有工资低于10000的人都被划分到同一区间。等频区间可能正好相反,所有工资高于50000的人都会被划分到50000这一区间中。这两种算法都忽略了实例所属的类型,落在正确区间里的偶然性很大。

我们对特征进行分箱后，需要对分箱后的每组（箱）进行woe编码，然后才能放进模型训练。

WOE编码

WOE(weight of evidence, 证据权重)

一种有监督的编码方式,将预测类别的集中度的属性作为编码的数值

优势 
　　将特征的值规范到相近的尺度上。 
　　(经验上讲,WOE的绝对值波动范围在0.1~3之间)。 
　　具有业务含义。 
　　 
缺点 
　　需要每箱中同时包含好、坏两个类别。

这里写图片描述

特征信息度

IV(Information Value), 衡量特征包含预测变量浓度的一种指标

这里写图片描述 
　特征信息度解构： 
　这里写图片描述 
　其中Gi,Bi表示箱i中好坏样本占全体好坏样本的比例。 
　WOE表示两类样本分布的差异性。 
　(Gi-Bi)：衡量差异的重要性。

　特征信息度的作用 
　选择变量：

非负指标
高IV表示该特征和目标变量的关联度高
目标变量只能是二分类
过高的IV,可能有潜在的风险
特征分箱越细,IV越高
常用的阈值有: 
<=0.02: 没有预测性,不可用 
0.02 to 0.1: 弱预测性 
0.1 to 0.2: 有一定的预测性 
0.2 +: 高预测性
注意上面说的IV是指一个变量里面所有箱的IV之和。

计算WOE和IV代码：

def CalcWOE(df, col, target):
    '''
    :param df: dataframe containing feature and target
    :param col: 注意col这列已经经过分箱了，现在计算每箱的WOE和总的IV。
    :param target: good/bad indicator
    :return: 返回每箱的WOE(字典类型）和总的IV之和。
    '''
    total = df.groupby([col])[target].count()
    total = pd.DataFrame({'total': total})
    bad = df.groupby([col])[target].sum()
    bad = pd.DataFrame({'bad': bad})
    regroup = total.merge(bad, left_index=True, right_index=True, how='left')
    regroup.reset_index(level=0, inplace=True)
    N = sum(regroup['total'])
    B = sum(regroup['bad'])
    regroup['good'] = regroup['total'] - regroup['bad']
    G = N - B
    regroup['bad_pcnt'] = regroup['bad'].map(lambda x: x*1.0/B)
    regroup['good_pcnt'] = regroup['good'].map(lambda x: x * 1.0 / G)
    regroup['WOE'] = regroup.apply(lambda x: np.log(x.good_pcnt*1.0/x.bad_pcnt),axis = 1)
    WOE_dict = regroup[[col,'WOE']].set_index(col).to_dict(orient='index')
    IV = regroup.apply(lambda x: (x.good_pcnt-x.bad_pcnt)*np.log(x.good_pcnt*1.0/x.bad_pcnt),axis = 1)
    IV = sum(IV)
    return {"WOE": WOE_dict, 'IV':IV}
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
那么可能有人会问，以上都是有监督的分箱，有监督的WOE编码，如何能将这些有监督的方法应用到预测集上呢？ 
　　 
　　我们观察下有监督的卡方分箱法和有监督的woe编码的计算公式不难发现，其计算结果都是以一个比值结果呈现（卡方分箱法：(坏样本数量-期望坏样本数量)/期望坏样本数量的比值形式；有监督的woe类似），比如我们发现预测集里面好坏样本不平衡，需要对坏样本进行一个欠采样或者是好样本进行过采样，只要是一个均匀采样，理论上这个有监督的卡方分箱的比值结果是不变的，其woe的比值结果也是不变的。即预测集上的卡方分组和woe编码和训练集上一样。 
　　 
　　那么，在训练集中我们对一个连续型变量进行分箱以后，对照这这个连续型变量每个值，如果这个值在某个箱中，那么就用这个箱子的woe编码代替他放进模型进行训练。

　　在预测集中类似，但是预测集中的这个连续型变量的某个值可能不在任一个箱中，比如在训练集中我对[x1,x2]分为一箱，[x3,x4]分为一箱，预测集中这个连续变量某个值可能为(x2+x3)/2即不在任意一箱中，如果把[x1,x2]分为一箱，那么这一箱的变量应该是x1<=x< x2；第二箱应该是x2<=x< x4等等。即预测集中连续变量某一个值大于等于第i-1个箱的最大值，小于第ｉ个箱子的最大值，那么这个变量就应该对应第ｉ个箱子。这样分箱就覆盖所有训练样本外可能存在的值。预测集中任意的一个值都可以找到对应的箱，和对应的woe编码。 
　　

def AssignBin(x, cutOffPoints):
    '''
    :param x: the value of variable
    :param cutOffPoints: 每组的最大值，也就是分割点
    :return: bin number, indexing from 0
    for example, if cutOffPoints = [10,20,30], if x = 7, return Bin 0. If x = 35, return Bin 3
    '''
    numBin = len(cutOffPoints) + 1
    if x<=cutOffPoints[0]:
        return 'Bin 0'
    elif x > cutOffPoints[-1]:
        return 'Bin {}'.format(numBin-1)
    else:
        for i in range(0,numBin-1):
            if cutOffPoints[i] < x <=  cutOffPoints[i+1]:
                return 'Bin {}'.format(i+1)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
　　 
　　如果我们发现分箱以后能完全能区分出好坏样本，那么得注意了这个连续变量会不会是个事后变量。

分箱的注意点

对于连续型变量做法:

使用ChiMerge进行分箱
如果有特殊值，把特殊值单独分为一组，例如把-1单独分为一箱。
计算这个连续型变量的每个值属于那个箱子，得出箱子编号。以所属箱子编号代替原始值。
def AssignBin(x, cutOffPoints):
    '''
    :param x: the value of variable
    :param cutOffPoints: the ChiMerge result for continous variable
    :return: bin number, indexing from 0
    for example, if cutOffPoints = [10,20,30], if x = 7, return Bin 0. If x = 35, return Bin 3
    '''
    numBin = len(cutOffPoints) + 1
    if x<=cutOffPoints[0]:
        return 'Bin 0'
    elif x > cutOffPoints[-1]:
        return 'Bin {}'.format(numBin-1)
    else:
        for i in range(0,numBin-1):
            if cutOffPoints[i] < x <=  cutOffPoints[i+1]:
                return 'Bin {}'.format(i+1)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
检查分箱以后每箱的bad_rate的单调性，如果不满足，那么继续进行相邻的两箱合并，知道bad_rate单调为止。(可以放宽到U型)
## determine whether the bad rate is monotone along the sortByVar
def BadRateMonotone(df, sortByVar, target):
    # df[sortByVar]这列数据已经经过分箱
    df2 = df.sort([sortByVar])
    total = df2.groupby([sortByVar])[target].count()
    total = pd.DataFrame({'total': total})
    bad = df2.groupby([sortByVar])[target].sum()
    bad = pd.DataFrame({'bad': bad})
    regroup = total.merge(bad, left_index=True, right_index=True, how='left')
    regroup.reset_index(level=0, inplace=True)
    combined = zip(regroup['total'],regroup['bad'])
    badRate = [x[1]*1.0/x[0] for x in combined]
    badRateMonotone = [badRate[i]<badRate[i+1] for i in range(len(badRate)-1)]
    Monotone = len(set(badRateMonotone))
    if Monotone == 1:
        return True
    else:
        return False
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
　　 上述过程是收敛的,因为当箱数为2时,bad rate自然单调

检查最大箱，如果最大箱里面数据数量占总数据的90%以上，那么弃用这个变量
def MaximumBinPcnt(df,col):
    N = df.shape[0]
    total = df.groupby([col])[col].count()
    pcnt = total*1.0/N
    return max(pcnt)
1
2
3
4
5
对于类别型变量：

当类别数较少时,原则上不需要分箱
否则，当类别较多时，以bad rate代替原有值，转成连续型变量再进行分箱计算。
def BadRateEncoding(df, col, target):
    '''
    :param df: dataframe containing feature and target
    :param col: the feature that needs to be encoded with bad rate, usually categorical type
    :param target: good/bad indicator
    :return: the assigned bad rate to encode the categorical fature
    '''
    total = df.groupby([col])[target].count()
    total = pd.DataFrame({'total': total})
    bad = df.groupby([col])[target].sum()
    bad = pd.DataFrame({'bad': bad})
    regroup = total.merge(bad, left_index=True, right_index=True, how='left')
    regroup.reset_index(level=0, inplace=True)
    regroup['bad_rate'] = regroup.apply(lambda x: x.bad*1.0/x.total,axis = 1)
    br_dict = regroup[[col,'bad_rate']].set_index([col]).to_dict(orient='index')
    badRateEnconding = df[col].map(lambda x: br_dict[x]['bad_rate'])
    return {'encoding':badRateEnconding, 'br_rate':br_dict}
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
否则， 检查最大箱，如果最大箱里面数据数量占总数据的90%以上，那么弃用这个变量

当某个或者几个类别的bad rate为0时,需要和最小的非0bad rate的箱进行合并。

### If we find any categories with 0 bad, then we combine these categories with that having smallest non-zero bad rate
def MergeBad0(df,col,target):
    '''
     :param df: dataframe containing feature and target
     :param col: the feature that needs to be calculated the WOE and iv, usually categorical type
     :param target: good/bad indicator
     :return: WOE and IV in a dictionary
     '''
    total = df.groupby([col])[target].count()
    total = pd.DataFrame({'total': total})
    bad = df.groupby([col])[target].sum()
    bad = pd.DataFrame({'bad': bad})
    regroup = total.merge(bad, left_index=True, right_index=True, how='left')
    regroup.reset_index(level=0, inplace=True)
    regroup['bad_rate'] = regroup.apply(lambda x: x.bad*1.0/x.total,axis = 1)
    regroup = regroup.sort_values(by = 'bad_rate')
    col_regroup = [[i] for i in regroup[col]]
    for i in range(regroup.shape[0]):
        col_regroup[1] = col_regroup[0] + col_regroup[1]
        col_regroup.pop(0)
        if regroup['bad_rate'][i+1] > 0:
            break
    newGroup = {}
    for i in range(len(col_regroup)):
        for g2 in col_regroup[i]:
            newGroup[g2] = 'Bin '+str(i)
    return newGroup
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
当该变量可以完全区分目标变量时,需要认真检查该变量的合理性。（可能是事后变量）
单变量分析

用IV检验该变量有效性（一般阈值区间在(0.0.2，0.8)）
iv_threshould = 0.02
## k,v分别表示col,col对应的这列的IV值。
varByIV = [k for k, v in var_IV.items() if v > iv_threshould]
## WOE_dict字典中包含字典。
WOE_encoding = []
for k in varByIV:
    if k in trainData.columns:
        trainData[str(k)+'_WOE'] = trainData[k].map(lambda x: WOE_dict[k][x]['WOE'])
        WOE_encoding.append(str(k)+'_WOE')
    elif k+str('_Bin') in trainData.columns:
        k2 = k+str('_Bin')
        trainData[str(k) + '_WOE'] = trainData[k2].map(lambda x: WOE_dict[k][x]['WOE'])
        WOE_encoding.append(str(k) + '_WOE')
    else:
        print "{} cannot be found in trainData"
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
连续变量bad rate的单调性(可以放宽到U型)
单一区间的占比不宜过高（一般不能超过90%，如果超过则弃用这个变量）
多变量分析

变量的两两相关性，当相关性高时,只能保留一个:

可以选择IV高的留下
或者选择分箱均衡的留下（后期评分得分会均匀）
#### we can check the correlation matrix plot
col_to_index = {WOE_encoding[i]:'var'+str(i) for i in range(len(WOE_encoding))}
#sample from the list of columns, since too many columns cannot be displayed in the single plot
corrCols = random.sample(WOE_encoding,15)
sampleDf = trainData[corrCols]
for col in corrCols:
    sampleDf.rename(columns = {col:col_to_index[col]}, inplace = True)
scatter_matrix(sampleDf, alpha=0.2, figsize=(6, 6), diagonal='kde')

#alternatively, we check each pair of independent variables, and selected the variabale with higher IV if they are highly correlated
compare = list(combinations(varByIV, 2))## 从varByIV随机的进行两两组合
removed_var = []
roh_thresould = 0.8
for pair in compare:
    (x1, x2) = pair
    roh = np.corrcoef([trainData[str(x1)+"_WOE"],trainData[str(x2)+"_WOE"]])[0,1]
    if abs(roh) >= roh_thresould:
        if var_IV[x1]>var_IV[x2]:## 选IV大的留下
            removed_var.append(x2)
        else:
            removed_var.append(x1)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
多变量分析：变量的多重共线性 
　通常用VIF来衡量，要求VIF<10: 
　这里写图片描述

import numpy as np
from sklearn.linear_model  import LinearRegression


selected_by_corr=[i for i in varByIv if i not in removed_var]
for i in range(len(selected_by_corr)):
    x0=trainData[selected_by_corr[i]+'_WOE']
    x0=np.array(x0)
    X_Col=[k+'_WOE' for k in selected_by_corr if k!=selected_by_corr[i]]
    X=trainData[X_Col]
    X=np.array(X)
    regr=LinearRegression()
    clr=regr.fit(X,x0)
    x_pred=clr.predit(X)
    R2=1-((x_pred-x0)**2).sum()/((x0-x0.mean())**2).sum()
    vif=1/(1-R2)
    print "The vif for {0} is {1}".format(selected_by_corr[i],vif)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
当发现vif>10时，需要逐一剔除变量，当剔除变量Xk时，发现vif<10时，此时剔除{Xi,Xk}中IV小的那个变量。 
通常情况下，计算vif这一步不是必须的，在进行单变量处理以后，放进逻辑回归模型进行训练预测，如果效果非常不好时，才需要做多变量分析，消除多重共线性。

本篇博文总结： 
　


　这里写图片描述
顶
0
 
踩
0
 
 
上一篇Hadoop-->Flume原理与应用
下一篇Hadoop-->HDFS原理总结
  相关文章推荐
? 评分卡模型开发-基于逻辑回归的标准评分卡实现
? 自然语言处理在“天猫精灵”的实践应用--姜飞俊
? 评分卡模型开发-主标尺设计及模型验证
? 蚂蜂窝大数据平台架构及Druid引擎实践--汪木铃
? 机器学习->监督学习->logistic回归,softMax回归
? Retrofit 从入门封装到源码解析
? 信用评分之二--信用评分中的评分卡中的A卡、B卡和C卡
? 程序员如何转型AI工程师
? 金融风控-->申请评分卡模型-->logisticRegression建模
? 深入探究Linux/VxWorks的设备树
? 齐次坐标的理解
? 使用QEMU搭建u-boot+Linux+NFS嵌入式开发环境
? 如何运用pyinstaller把Python中的.py文件转换成.exe可执行文件
? 金融风控-->申请评分卡模型-->申请评分卡介绍
? cygwin的在线安装最快的镜像，http://mirrors.163.com/cygwin/，注意路劲不要修改不用手动的去输入64位还是32为的
? 信用标准评分卡模型开发及实现

查看评论
1楼 zhow123 2017-08-16 16:29发表 [回复] 您好，数据集可以分享一下吗？我想拿来跑一下，练练手
您还没有登录,请[登录]或[注册]
* 以上用户言论只代表其个人观点，不代表CSDN网站的观点或立场
个人资料
访问我的空间  
村头陶员外
 
 3
访问：20819次
积分：812
等级： 
排名：千里之外
原创：63篇转载：4篇译文：0篇评论：9条
文章搜索

 搜索
文章分类
机器学习--监督学习(3)
机器学习--推荐系统(8)
机器学习--统计学基(0)
机器学习-统计学基础(3)
机器学习-无监督学习(2)
深度学习-神经网络(5)
python爬虫(8)
机器学习-集成学习(7)
python可视化(1)
机器学习-数据预处理(3)
机器学习-特征选择(1)
机器学习-模型选择(1)
比赛成绩统计系统(0)
数据挖掘比赛(3)
金融风控(7)
hadoop(2)
机器学习---数学(4)
机器学习-python(3)
机器学习-特征降维(2)
算法基础(9)
生活学习总结(1)
文章存档
2017年09月(4)
2017年08月(16)
2017年07月(22)
2017年06月(10)
2017年05月(4)
展开
阅读排行
机器学习->统计学基础->贝叶斯估计,最大似然估计(MLE),最大后验估计(MAP)(1088)
金融风控-->申请评分卡模型-->申请评分卡介绍(1028)
金融风控-->申请评分卡模型-->特征工程（特征分箱，WOE编码）(984)
机器学习->推荐系统->冷启动问题(873)
金融风控-->申请评分卡模型-->logisticRegression建模(847)
机器学习->推荐系统->基于图的推荐算法(PersonalRank)(776)
金融风控-->客户流失预警模型-->金融数据分析(668)
生活学习总结-->2017-->上半年总结以及下半年计划(661)
机器学习-->深度学习-->卷积神经网络(CNN)(659)
分布式爬虫-->1 多线程多进程爬虫(583)
评论排行
机器学习-->特征降维方法总结(3)
生活学习总结-->2017-->上半年总结以及下半年计划(2)
金融风控-->申请评分卡模型-->申请评分卡介绍(1)
机器学习->统计学基础->贝叶斯估计,最大似然估计(MLE),最大后验估计(MAP)(1)
金融风控-->申请评分卡模型-->logisticRegression建模(1)
金融风控-->申请评分卡模型-->特征工程（特征分箱，WOE编码）(1)
Leetcode笔记-->第一周(0)
机器学习->推荐系统->冷启动问题(0)
机器学习->监督学习->logistic回归,softMax回归(0)
机器学习->推荐系统->给用户推荐标签(0)
推荐文章
* CSDN新版博客feed流内测用户征集令
* Android检查更新下载安装
* 动手打造史上最简单的 Recycleview 侧滑菜单
* TCP网络通讯如何解决分包粘包问题
* SDCC 2017之大数据技术实战线上峰会
* 快速集成一个视频直播功能
最新评论
生活学习总结-->2017-->上半年总结以及下半年计划
村头陶员外: @xidianliutingting:谢谢
生活学习总结-->2017-->上半年总结以及下半年计划
心雨心辰: 加油！
机器学习-->特征降维方法总结
夜中的风: 好的，谢谢。。
机器学习-->特征降维方法总结
村头陶员外: @YeZhongDeFeng:这是一个非常著名的数据集，你百度或者谷歌“iris.data”，就能搜...
机器学习-->特征降维方法总结
夜中的风: 楼主你好，想问下，data = pd.read_csv(&#39;iris.data&#39;, h...
金融风控-->申请评分卡模型-->申请评分卡介绍
CZ626626: 你好，我最近研究你的信用卡打分模型，在得到模型以后的具体打分的代码有吗？我想学习一下，谢谢
金融风控-->申请评分卡模型-->特征工程（特征分箱，WOE编码）
zhow123: 您好，数据集可以分享一下吗？我想拿来跑一下，练练手
金融风控-->申请评分卡模型-->logisticRegression建模
靖凡无所畏惧: 厉害了，我这个小白都长见识了
机器学习->统计学基础->贝叶斯估计,最大似然估计(MLE),最大后验估计(MAP)
Xu_Wanfeng: 写的很好吧，谢谢分享


公司简介|招贤纳士|广告服务|联系方式|版权声明|法律顾问|问题报告|合作伙伴|论坛反馈
网站客服杂志客服微博客服webmaster@csdn.net400-660-0108|北京创新乐知信息技术有限公司 版权所有|江苏知之为计算机有限公司|江苏乐知网络技术有限公司
京 ICP 证 09002463 号|Copyright ? 1999-2017, CSDN.NET, All Rights Reserved GongshangLogo


